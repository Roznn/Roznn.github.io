@INPROCEEDINGS{Panahi2023,
  ABSTRACT = {Point set registration is critical in many applications such as computer vision, pattern recognition, or in fields like robotics and medical imaging. This paper focuses on reformulating point set registration using Gaussian Mixture Models while considering attributes associated with each point. Our approach introduces class score vectors as additional features to the spatial data information. By incorporating these attributes, we enhance the optimization process by penalizing incorrect matching terms. Experimental results show that our approach with class scores outperforms the original algorithm in both accuracy and speed.},
  AUTHOR = {Panahi, Solmaz and Chopin, Jeremy and Ulicny, Matej and Dahyot, Rozenn},
  LOCATION = {Galway, Ireland},
  URL = {https://zenodo.org/records/8205096/files/Improving_GMM_registration_with_class_encodings.pdf},
  BOOKTITLE = {Irish Machine Vision and Image Processing (IMVIP 2023)},
  DATE = {2023},
  DOI = {10.5281/zenodo.8205096},
  KEYWORDS = {registration,GMM,class encoding},
  NOTE = {https://github.com/solmak97/GMMReg_Extension},
  TITLE = {Improving GMM registration with class encoding},
}

@INPROCEEDINGS{KoteyInterSpeech2023,
  ABSTRACT = {Podcasts are a rich storytelling medium of long diverse conversations. Typically, listeners preview an episode through an audio clip, before deciding to consume the content. An automatic system that produces promotional clips, by supporting acoustic queries would greatly benefit podcasters. Previous text based methods do not use the acoustic signal directly or incorporate acoustic defined queries. Therefore, we propose a query based summarization approach, to produce audio clip summaries from podcast data. Leveraging unsupervised clustering methods, we apply our framework to the Spotify podcasts dataset. Audio signals are transformed into acoustic word embeddings, along with a pre-selected candidate query. We initiate the cluster centroids with the query vector and obtain the final snippets by computing a global and local similarity score. Additionally, we apply our framework to the AMI meeting dataset and demonstrate how audio can successfully be utilized to perform summarization.},
  AUTHOR = {Kotey, Samantha and Dahyot, Rozenn and Harte, Naomi},
  LOCATION = {Dublin, Ireland},
  URL = {https://www.isca-speech.org/archive/pdfs/interspeech_2023/kotey23_interspeech.pdf},
  BOOKTITLE = {Proceedings INTERSPEECH 2023},
  DATE = {2023-08},
  DOI = {10.21437/Interspeech.2023-864},
  KEYWORDS = {query-based summarization,unsupervised speech summarization,clustering,acoustic word embeddings},
  PAGES = {1483--1487},
  TITLE = {Query Based Acoustic Summarization for Podcasts},
}

@REPORT{ulicny2023combining,
  ABSTRACT = {We propose a pipeline for combined multi-class object geolocation and height estimation from street level RGB imagery, which is considered as a single available input data modality. Our solution is formulated via Markov Random Field optimization with deterministic output. The proposed technique uses image metadata along with coordinates of objects detected in the image plane as found by a custom-trained Convolutional Neural Network. Computing the object height using our methodology, in addition to object geolocation, has negligible effect on the overall computational cost. Accuracy is demonstrated experimentally for water drains and road signs on which we achieve average elevation estimation error lower than 20cm.},
  AUTHOR = {Ulicny, Matej and Krylov, Vladimir A. and Connelly, Julie and Dahyot, Rozenn},
  URL = {https://arxiv.org/pdf/2305.08232.pdf},
  DATE = {2023},
  DOI = {10.48550/arXiv.2305.08232},
  EPRINT = {2305.08232},
  EPRINTCLASS = {cs.CV},
  EPRINTTYPE = {arXiv},
  TITLE = {Combining geolocation and height estimation of objects from street level imagery},
  TYPE = {techreport},
}

@ARTICLE{CHOPIN2023103744,
  ABSTRACT = {Deep learning based pipelines for semantic segmentation often ignore structural information available on annotated images used for training. We propose a novel post-processing module enforcing structural knowledge about the objects of interest to improve segmentation results provided by deep neural networks (DNNs). This module corresponds to a “many-to-one-or-none” inexact graph matching approach, and is formulated as a quadratic assignment problem. Our approach is compared to a DNN-based segmentation on two public datasets, one for face segmentation from 2D RGB images (FASSEG), and the other for brain segmentation from 3D MRIs (IBSR). Evaluations are performed using two types of structural information: distances and directional relations that are user defined, this choice being a hyper-parameter of our proposed generic framework. On FASSEG data, results show that our module improves accuracy of the DNN by about 6.3\% i.e. the Hausdorff distance (HD) decreases from 22.11 to 20.71 on average. With IBSR data, the improvement is of 51\% better accuracy with HD decreasing from 11.01 to 5.4. Finally, our approach is shown to be resilient to small training datasets that often limit the performance of deep learning methods: the improvement increases as the size of the training dataset decreases.},
  AUTHOR = {Chopin, Jeremy and Fasquel, Jean-Baptiste and Mouchère, Harold and Dahyot, Rozenn and Bloch, Isabelle},
  URL = {https://arxiv.org/pdf/2301.07468.pdf},
  DATE = {2023},
  DOI = {10.1016/j.cviu.2023.103744},
  ISSN = {1077-3142},
  JOURNALTITLE = {Computer Vision and Image Understanding},
  KEYWORDS = {Graph matching,Deep learning,Image segmentation,Volume segmentation,Quadratic assignment problem},
  PAGES = {103744},
  TITLE = {Model-based inexact graph matching on top of DNNs for semantic scene understanding},
}

@REPORT{Dahyot_PCC2022,
  ABSTRACT = {We propose to directly compute classification estimates by learning features encoded with their class scores. Our resulting model has a encoder-decoder structure suitable for supervised learning, it is computationally efficient and performs well for classification on several datasets.},
  AUTHOR = {Dahyot, Rozenn},
  PUBLISHER = {arXiv},
  URL = {https://arxiv.org/pdf/2210.12746.pdf},
  DATE = {2022},
  DOI = {10.48550/ARXIV.2210.12746},
  KEYWORDS = {Supervised Learning,PCA,classification,metric learning,deep learning,class encoding},
  TITLE = {Principal Component Classification},
  TYPE = {techreport},
}

@INPROCEEDINGS{KoteySLT2023,
  ABSTRACT = {Podcast transcripts are long spoken documents of conversational dialogue. Challenging to summarize, podcasts cover a diverse range of topics, vary in length, and have uniquely different linguistic styles. Previous studies in podcast summarization have generated short, concise dialogue summaries. In contrast, we propose a method to generate long fine-grained summaries, which describe details of sub-topic narratives. Leveraging a readability formula, we curate a data subset to train a long sequence transformer for abstractive summarization. Through text segmentation, we filter the evaluation data and exclude specific segments of text. We apply the model to segmented data, producing different types of fine grained summaries. We show that appropriate filtering creates comparable results on ROUGE and serves as an alternative method to truncation. Experiments show our model outperforms previous studies on the Spotify podcast dataset when tasked with generating longer sequences of text.},
  AUTHOR = {Kotey, Samantha and Dahyot, Rozenn and Harte, Naomi},
  URL = {https://roznn.github.io/PDF/STL2022Kotey.pdf},
  BOOKTITLE = {2022 IEEE Spoken Language Technology Workshop (SLT)},
  DATE = {2023-01},
  DOI = {10.1109/SLT54892.2023.10022829},
  PAGES = {647--654},
  TITLE = {Fine Grained Spoken Document Summarization Through Text Segmentation},
}

@ARTICLE{ULICNY2022108707,
  ABSTRACT = {Convolutional neural networks (CNNs) learn filters in order to capture local correlation patterns in feature space. We propose to learn these filters as combinations of preset spectral filters defined by the Discrete Cosine Transform (DCT). Our proposed DCT-based harmonic blocks replace conventional convolutional layers to produce partially or fully harmonic versions of new or existing CNN architectures. Using DCT energy compaction properties, we demonstrate how the harmonic networks can be efficiently compressed by truncating high-frequency information in harmonic blocks thanks to the redundancies in the spectral domain. We report extensive experimental validation demonstrating benefits of the introduction of harmonic blocks into state-of-the-art CNN models in image classification, object detection and semantic segmentation applications.},
  AUTHOR = {Ulicny, Matej and Krylov, Vladimir A. and Dahyot, Rozenn},
  URL = {https://arxiv.org/pdf/2001.06570.pdf},
  DATE = {2022},
  DOI = {10.1016/j.patcog.2022.108707},
  ISSN = {0031-3203},
  JOURNALTITLE = {Pattern Recognition},
  NOTE = {arXiv.2001.06570 Github: https://github.com/matej-ulicny/harmonic-networks},
  PAGES = {1--12},
  TITLE = {Harmonic Convolutional Networks based on Discrete Cosine Transform},
  VOLUME = {129},
}

@INPROCEEDINGS{ChopinICPRAI2022a,
  ABSTRACT = {Deep learning based pipelines for semantic segmentation often ignore structural information available on annotated images used for training. We propose a novel post-processing module enforcing structural knowledge about the objects of interest to improve segmentation results provided by deep learning. This module corresponds to a “manyto- one-or-none” inexact graph matching approach, and is formulated as a quadratic assignment problem. Using two standard measures for evaluation, we show experimentally that our pipeline for segmentation of 3D MRI data of the brain outperforms the baseline CNN (U-Net) used alone. In addition, our approach is shown to be resilient to small training datasets that often limit the performance of deep learning.},
  AUTHOR = {Chopin, J. and Fasquel, J.-B. and Mouchere, H. and Dahyot, R. and Bloch, I.},
  EDITOR = {El Yacoubi, Mounîm and Granger, Eric and Yuen, Pong Chi and Pal, Umapada and Vincent, Nicole},
  LOCATION = {Paris, France},
  PUBLISHER = {Springer International Publishing},
  URL = {https://hal.science/hal-03633029/document},
  BOOKTITLE = {Pattern Recognition and Artificial Intelligence},
  DATE = {2022-06},
  DOI = {10.1007/978-3-031-09037-0_15},
  ISBN = {978-3-031-09037-0},
  NOTE = {hal-03633029},
  PAGES = {173--184},
  TITLE = {Improving semantic segmentation with graph-based structural knowledge},
}

@INPROCEEDINGS{ChopinICPRAI2022b,
  ABSTRACT = {The paper addresses the fundamental task of semantic image analysis by exploiting structural information (spatial relationships between image regions). We propose to perform such semantic image analysis by combining a deep neural network (CNN) with graph matching where graphs encode efficiently structural information related to regions segmented by the CNN. Our novel approach solves the quadratic assignment problem (QAP) sequentially for matching graphs. The optimal sequence for graph matching is conveniently defined using reinforcementlearning (RL) based on the region membership probabilities produced by the CNN and their structural relationships. Our RL based strategy for solving QAP sequentially allows us to significantly reduce the combinatioral complexity for graph matching. Preliminary experiments are performed on both a synthetic dataset and a public dataset dedicated to the semantic segmentation of face images. Results show that the proposed RL-based ordering dramatically outperforms random ordering, and that our strategy is about 386 times faster than a global QAP-based approach, while preserving similar segmentation accuracy.},
  AUTHOR = {Chopin, J. and Fasquel, J.-B. and Mouchere, H. and Dahyot, R. and Bloch, I.},
  EDITOR = {El Yacoubi, Mounîm and Granger, Eric and Yuen, Pong Chi and Pal, Umapada and Vincent, Nicole},
  LOCATION = {Paris, France},
  PUBLISHER = {Springer International Publishing},
  URL = {https://hal.science/hal-03633036/document},
  BOOKTITLE = {Pattern Recognition and Artificial Intelligence},
  DATE = {2022-06},
  DOI = {10.1007/978-3-031-09037-0_5},
  ISBN = {978-3-031-09037-0},
  NOTE = {hal-03633036},
  PAGES = {47--58},
  TITLE = {QAP Optimisation with Reinforcement Learning for Faster Graph Matching in Sequential Semantic Image Analysis},
}

@INPROCEEDINGS{karaali2022drvnet,
  ABSTRACT = {Accurate retinal vessel segmentation is an important task for many computer-aided diagnosis systems. Yet, it is still a challenging problem due to the complex vessel structures of an eye. Numerous vessel segmentation methods have been proposed recently, however more research is needed to deal with poor segmentation of thin and tiny vessels. To address this, we propose a new deep learning pipeline combining the efficiency of residual dense net blocks and, residual squeeze and excitation blocks. We validate experimentally our approach on three datasets and show that our pipeline outperforms current state of the art techniques on the sensitivity metric relevant to assess capture of small vessels.},
  AUTHOR = {Karaali, Ali and Dahyot, Rozenn and Sexton, Donal J.},
  EDITOR = {El Yacoubi, Mounîm and Granger, Eric and Yuen, Pong Chi and Pal, Umapada and Vincent, Nicole},
  LOCATION = {Paris, France},
  PUBLISHER = {Springer International Publishing},
  URL = {https://arxiv.org/pdf/2111.04739.pdf},
  BOOKTITLE = {Pattern Recognition and Artificial Intelligence},
  DATE = {2022-06},
  DOI = {10.1007/978-3-031-09037-0_17},
  EPRINTCLASS = {eess.IV},
  EPRINTTYPE = {arXiv},
  ISBN = {978-3-031-09037-0},
  NOTE = {Github https://github.com/alikaraali/DR-VNet, ArXivDOI:10.48550/arXiv.2111.04739},
  TITLE = {DR-VNet: Retinal Vessel Segmentation via Dense Residual UNet},
}

@INPROCEEDINGS{ChaoImvip2021,
  ABSTRACT = {We propose an approach for geolocating assets from street view imagery by improving the quality of the metadata associated with the images using Structure from Motion, and by using contextual geographic information extracted from OpenStreetMap. Our pipeline is validated experimentally against the state of the art approaches for geotagging traffic lights.},
  AUTHOR = {Liu, C.-J. and Ulicny, Matej and Manzke, Michael and Dahyot, Rozenn},
  URL = {https://arxiv.org/pdf/2108.06302.pdf},
  BOOKTITLE = {Irish Machine Vision and Image Processing (IMVIP 2021)},
  DATE = {2021},
  DOI = {10.48550/arXiv.2108.06302},
  EPRINTTYPE = {arXiv},
  TITLE = {Context Aware Object Geotagging},
}

@ARTICLE{McDonnell2021,
  ABSTRACT = {Blendshape facial rigs are used extensively in the industry for facial animation of virtual humans. However, storing and manipulating large numbers of facial meshes (blendshapes) is costly in terms of memory and computation for gaming applications. Blendshape rigs are comprised of sets of semantically-meaningful expressions, which govern how expressive the character will be, often based on Action Units from the Facial Action Coding System (FACS). However, the relative perceptual importance of blendshapes has not yet been investigated. Research in Psychology and Neuroscience has shown that our brains process faces differently than other objects so we postulate that the perception of facial expressions will be feature-dependent rather than based purely on the amount of movement required to make the expression. Therefore, we believe that perception of blendshape visibility will not be reliably predicted by numerical calculations of the difference between the expression and the neutral mesh. In this paper, we explore the noticeability of blendshapes under different activation levels, and present new perceptually-based models to predict perceptual importance of blendshapes. The models predict visibility based on commonly-used geometry and image-based metrics.},
  AUTHOR = {McDonnell, Rachel and Zibrek, Katja and Carrigan, Emma and Dahyot, Rozenn},
  URL = {https://roznn.github.io/facial-blendshapes/CAG2021.pdf},
  DATE = {2021},
  DOI = {10.1016/j.cag.2021.07.022},
  ISSN = {0097-8493},
  JOURNALTITLE = {Computers \& Graphics},
  KEYWORDS = {facial action unit,perception,virtual character},
  NOTE = {Winner 2022 Graphics Replicability Stamp Initiative (GRSI) best paper award; Github: https://github.com/Roznn/facial-blendshapes},
  PAGES = {81--92},
  TITLE = {Model for predicting perception of facial action unit activation using virtual humans},
  VOLUME = {100},
}

@INPROCEEDINGS{alghamdi2021sliced,
  ABSTRACT = {We propose a new method with L2 distance that maps one N-dimensional distribution to another, taking into account available information about correspondences. We solve the high-dimensional problem in 1D space using an iterative projection approach. To show the potentials of this mapping, we apply it to colour transfer between two images that exhibit overlapped scenes. Experiments show quantitative and qualitative competitive results as compared with the state of the art colour transfer methods.},
  AUTHOR = {Alghamdi, Hana and Dahyot, Rozenn},
  URL = {https://arxiv.org/pdf/2102.09297.pdf},
  BOOKTITLE = {29th European Signal Processing Conference (EUSIPCO)},
  DATE = {2021},
  DOI = {10.23919/EUSIPCO54536.2021.9616260},
  EPRINT = {2102.09297},
  EPRINTCLASS = {cs.CV},
  EPRINTTYPE = {arXiv},
  NOTE = {https://eurasip.org/Proceedings/Eusipco/Eusipco2021/pdfs/0000671.pdf},
  PAGES = {671--675},
  TITLE = {Sliced L2 Distance for Colour Grading},
}

@INPROCEEDINGS{ulicny2020tensor,
  ABSTRACT = {We show how parameter redundancy in Convolutional Neural Network (CNN) filters can be effectively reduced by pruning in spectral domain. Specifically, the representation extracted via Discrete Cosine Transform (DCT) is more conducive for pruning than the original space. By relying on a combination of weight tensor reshaping and reordering we achieve high levels of layer compression with just minor accuracy loss. Our approach is applied to compress pretrained CNNs and we show that minor additional fine-tuning allows our method to recover the original model performance after a significant parameter reduction. We validate our approach on ResNet-50 and MobileNet-V2 architectures for ImageNet classification task.},
  AUTHOR = {Ulicny, Matej and Krylov, Vladimir A. and Dahyot, Rozenn},
  URL = {https://arxiv.org/pdf/2010.12110.pdf},
  BOOKTITLE = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  DATE = {2021},
  DOI = {10.1109/ICASSP39728.2021.9413944},
  EPRINT = {2010.12110},
  EPRINTCLASS = {cs.LG},
  EPRINTTYPE = {arXiv},
  NOTE = {Github: https://github.com/matej-ulicny/reorder-cnn-compression},
  PAGES = {3930--3934},
  TITLE = {Tensor Reordering for CNN Compression},
}

@ARTICLE{10.1145/3403572,
  ABSTRACT = {Next generation of embedded Information and Communication Technology (ICT) systems are interconnected and collaborative systems able to perform autonomous tasks. The remarkable expansion of the embedded ICT market, together with the rise and breakthroughs of Artificial Intelligence (AI), have put the focus on the Edge as it stands as one of the keys for the next technological revolution: the seamless integration of AI in our daily life. However, training and deployment of custom AI solutions on embedded devices require a fine-grained integration of data, algorithms, and tools to achieve high accuracy and overcome functional and non-functional requirements. Such integration requires a high level of expertise that becomes a real bottleneck for small and medium enterprises wanting to deploy AI solutions on the Edge, which, ultimately, slows down the adoption of AI on applications in our daily life.In this work, we present a modular AI pipeline as an integrating framework to bring data, algorithms, and deployment tools together. By removing the integration barriers and lowering the required expertise, we can interconnect the different stages of particular tools and provide a modular end-to-end development of AI products for embedded devices. Our AI pipeline consists of four modular main steps: (i) data ingestion, (ii) model training, (iii) deployment optimization, and (iv) the IoT hub integration. To show the effectiveness of our pipeline, we provide examples of different AI applications during each of the steps. Besides, we integrate our deployment framework, Low-Power Deep Neural Network (LPDNN), into the AI pipeline and present its lightweight architecture and deployment capabilities for embedded devices. Finally, we demonstrate the results of the AI pipeline by showing the deployment of several AI applications such as keyword spotting, image classification, and object detection on a set of well-known embedded platforms, where LPDNN consistently outperforms all other popular deployment frameworks.},
  AUTHOR = {Prado, Miguel De and Su, Jing and Saeed, Rabia and Keller, Lorenzo and Vallez, Noelia and Anderson, Andrew and Gregg, David and Benini, Luca and Llewellynn, Tim and Ouerhani, Nabil and Dahyot, Rozenn and Pazos, Nuria},
  LOCATION = {New York, NY, USA},
  PUBLISHER = {Association for Computing Machinery},
  URL = {https://arxiv.org/pdf/1901.05049.pdf},
  DATE = {2020-08},
  DOI = {10.1145/3403572},
  ISSN = {2691-1914},
  JOURNALTITLE = {ACM Trans. Internet Things},
  KEYWORDS = {deep learning,AI pipeline,keyword spotting,fragmentation},
  NUMBER = {4},
  TITLE = {Bonseyes AI Pipeline—Bringing AI to You: End-to-End Integration of Data, Algorithms, and Deployment Tools},
  VOLUME = {1},
}

@INPROCEEDINGS{chopin:hal-02882043,
  ABSTRACT = {We propose a method for semantic image segmentation, combining a deep neural network and spatial relationships between image regions, encoded in a graph representation of the scene. Our proposal is based on inexact graph matching, applied to the output of a deep neural network. The proposed method is evaluated on a public dataset used for segmentation of images of faces. Preliminary results show that, in terms of IoU of region bounding boxes, the use of spatial relationships lead to an improvement of 2.4 percent in average, and up to 24.4 percent for some regions.},
  AUTHOR = {Chopin, J. and Fasquel, J.-B. and Mouchere, H. and Bloch, I. and Dahyot, R.},
  LOCATION = {Angers, France},
  URL = {https://hal.science/hal-02882043/document},
  BOOKTITLE = {Rencontres des Jeunes Chercheurs en Intelligence Artificielle (RJCIA 2020)},
  DATE = {2020-06},
  NOTE = {https://hal.archives-ouvertes.fr/hal-02882043},
  TITLE = {Methode d'analyse semantique d'images combinant apprentissage profond et relations structurelles par appariement de graphes},
}

@ARTICLE{Smile2020,
  ABSTRACT = {Estimation of the Digital Surface Model (DSM) and building heights from single-view aerial imagery is a challenging inherently ill-posed problem that we address in this paper by resorting to machine learning. We propose an end-to-end trainable convolutional-deconvolutional deep neural network architecture that enables learning mapping from a single aerial imagery to a DSM for analysis of urban scenes. We perform multisensor fusion of aerial optical and aerial light detection and ranging (Lidar) data to prepare the training data for our pipeline. The dataset quality is key to successful estimation performance. Typically, a substantial amount of misregistration artifacts are present due to georeferencing/projection errors, sensor calibration inaccuracies, and scene changes between acquisitions. To overcome these issues, we propose a registration procedure to improve Lidar and optical data alignment that relies on Mutual Information, followed by Hough transform-based validation step to adjust misregistered image patches. We validate our building height estimation model on a high-resolution dataset captured over central Dublin, Ireland: Lidar point cloud of 2015 and optical aerial images from 2017. These data allow us to validate the proposed registration procedure and perform 3D model reconstruction from single-view aerial imagery. We also report state-of-the-art performance of our proposed architecture on several popular DSM estimation datasets},
  AUTHOR = {Liu, C.-J. and Krylov, V. A. and Kane, P. and Kavanagh, G. and Dahyot, R.},
  PUBLISHER = {{MDPI} {AG}},
  URL = {https://www.mdpi.com/2072-4292/12/17/2719/pdf},
  DATE = {2020-08},
  DOI = {10.3390/rs12172719},
  JOURNALTITLE = {Remote Sensing},
  NOTE = {Github: https://github.com/speed8928/IMELE},
  NUMBER = {17},
  PAGES = {2719},
  TITLE = {IM2ELEVATION: Building Height Estimation from Single-View Aerial Imagery},
  VOLUME = {12},
}

@INPROCEEDINGS{10.1145/3424636.3426904,
  ABSTRACT = {Blendshape facial rigs are used extensively in the industry for facial animation of virtual humans. However, storing and manipulating large numbers of facial meshes is costly in terms of memory and computation for gaming applications, yet the relative perceptual importance of blendshapes has not yet been investigated. Research in Psychology and Neuroscience has shown that our brains process faces differently than other objects, so we postulate that the perception of facial expressions will be feature-dependent rather than based purely on the amount of movement required to make the expression. In this paper, we explore the noticeability of blendshapes under different activation levels, and present new perceptually based models to predict perceptual importance of blendshapes. The models predict visibility based on commonly-used geometry and image-based metrics.},
  AUTHOR = {Carrigan, Emma and Zibrek, Katja and Dahyot, Rozenn and McDonnell, Rachel},
  LOCATION = {Virtual Event, SC, USA},
  PUBLISHER = {Association for Computing Machinery},
  URL = {https://dl.acm.org/doi/pdf/10.1145/3424636.3426904},
  BOOKTITLE = {Motion, Interaction and Games},
  DATE = {2020},
  DOI = {10.1145/3424636.3426904},
  ISBN = {9781450381710},
  KEYWORDS = {blendshapes,perception,action units,linear model},
  NOTE = {Awarded Best Short Paper Award MIG2020 Github: https://roznn.github.io/facial-blendshapes/},
  SERIES = {MIG '20},
  TITLE = {Investigating Perceptually Based Models to Predict Importance of Facial Blendshapes},
}

@INPROCEEDINGS{DBLP:journals/corr/abs-2006-09208,
  ABSTRACT = {We propose a new method with Nadaraya-Watson that maps one N-dimensional distribution to another, taking into account available information about correspondences. We extend the 2D/3D problem to higher dimensions by encoding overlapping neighborhoods of data points and solve the high-dimensional problem in 1D space using an iterative projection approach. To show the potentials of this mapping, we apply it to colour transfer between two images that exhibit overlapped scenes. Experiments show quantitative and qualitative improvements over the previous state of the art colour transfer methods.},
  AUTHOR = {Alghamdi, Hana and Dahyot, Rozenn},
  URL = {https://arxiv.org/pdf/2006.09208.pdf},
  BOOKTITLE = {IEEE 22nd International Workshop on Multimedia Signal Processing (MMSP)},
  DATE = {2020},
  DOI = {10.1109/MMSP48831.2020.9287097},
  NOTE = {Github: https://github.com/leshep/INWDT},
  PAGES = {1--6},
  TITLE = {Iterative Nadaraya-Watson Distribution Transfer for Colour Grading},
}

@INPROCEEDINGS{DBLP:journals/corr/abs-2005-09015,
  ABSTRACT = {We propose a new colour transfer method with Optimal Transport (OT) to transfer the colour of a sourceimage to match the colour of a target image of the same scene that may exhibit large motion changes betweenimages. By definition OT does not take into account any available information about correspondences whencomputing the optimal solution. To tackle this problem we propose to encode overlapping neighborhoodsof pixels using both their colour and spatial correspondences estimated using motion estimation. We solvethe high dimensional problem in 1D space using an iterative projection approach. We further introducesmoothing as part of the iterative algorithms for solving optimal transport namely Iterative DistributionTransport (IDT) and its variant the Sliced Wasserstein Distance (SWD). Experiments show quantitative andqualitative improvements over previous state of the art colour transfer methods.},
  AUTHOR = {Alghamdi, Hana and Dahyot, Rozenn},
  URL = {https://arxiv.org/pdf/2005.09015.pdf},
  BOOKTITLE = {Irish Machine Vision and Image Processing (IMVIP 2020)},
  DATE = {2020},
  EPRINT = {2005.09015},
  EPRINTTYPE = {arXiv},
  NOTE = {Best Paper Award, Book Open Access http://research.thea.ie/handle/20.500.12065/3429},
  TITLE = {Patch based Colour Transfer using {SIFT} Flow},
}

@INPROCEEDINGS{ReemIMVIP2020,
  AUTHOR = {Aljuaidi, R. and Dahyot, R.},
  URL = {http://research.thea.ie/handle/20.500.12065/3429},
  BOOKTITLE = {Irish Machine Vision and Image Processing (IMVIP 2020)},
  DATE = {2020},
  NOTE = {Book Open Access http://research.thea.ie/handle/20.500.12065/3429},
  TITLE = {Efficient Visual Place Retrieval System Using Google Street View},
}

@INPROCEEDINGS{YadavIMVIP2020,
  ABSTRACT = {Visual spectrum camera is a primary sensor in an automated driving system. It provides a high information density at a low cost. Visual perception is extensively studied in the literature and it is a mature component deployed in existing commercial vehicles. Its main disadvantage is the performance degradation in low light scenarios. Thermal cameras are increasingly being used to complement cameras for dark conditions like night time or driving through a tunnel. In this paper, we explore CNN based fusion architecture for object detection. We explore two automotive datasets which provide data for both these sensors namely KAIST multispectral pedestrian dataset and FLIR thermal object detection dataset. We train baseline Faster- RCNN models for color only and thermal only models on KAIST dataset. Color model outperforms Thermal in day conditions and Thermal model outperforms color in night conditions illustrating their complementary nature. We construct a simple mid-level CNN fusion architecture which performs significantly better than the baseline models. We observe an improvement of 0.62\% in miss rate compared to existing methods. We also explored the more recent FLIR dataset. Because of the vastly different resolution, aspect ratio and field of view of the color and thermal images provided, our simple fusion architecture did not perform well pointing out the need for further research in this area.},
  AUTHOR = {Yadav, R. and Samir, A. and Rashed, H. and Yogamani, S. and Dahyot, R.},
  URL = {https://research.thea.ie/bitstream/handle/20.500.12065/3429/IMVIP2020Proceedings.pdf},
  BOOKTITLE = {Irish Machine Vision and Image Processing (IMVIP 2020)},
  DATE = {2020},
  NOTE = {Book Open Access http://research.thea.ie/handle/20.500.12065/3429},
  TITLE = {CNN based Color and Thermal Image Fusion for Object Detection in Automated Driving},
}

@INPROCEEDINGS{9286611,
  ABSTRACT = {We propose a method for semantic image segmentation, combining a deep neural network and spatial relationships between image regions, encoded in a graph representation of the scene. Our proposal is based on inexact graph matching, formulated as a quadratic assignment problem applied to the output of the neural network. The proposed method is evaluated on a public dataset used for segmentation of images of faces, and compared to the U-Net deep neural network that is widely used for semantic segmentation. Preliminary results show that our approach is promising. In terms of Intersection-over-Union of region bounding boxes, the improvement is of 2.4\% in average, compared to U-Net, and up to 24.4\% for some regions. Further improvements are observed when reducing the size of the training dataset (up to 8.5\% in average).},
  AUTHOR = {{Chopin}, J. and {Fasquel}, J.B. and {Mouchere}, H. and {Dahyot}, R. and {Bloch}, I.},
  URL = {https://hal.science/hal-02916165/document},
  BOOKTITLE = {2020 Tenth International Conference on Image Processing Theory, Tools and Applications (IPTA)},
  DATE = {2020-11},
  DOI = {10.1109/IPTA50016.2020.9286611},
  ISSN = {2154-512X},
  KEYWORDS = {Computer vision;Deep learning;Inexact graph matching;Quadratic assignment problem},
  NOTE = {https://github.com/Jeremy-Chopin/FASSEG-instances},
  PAGES = {1--6},
  TITLE = {Semantic image segmentation based on spatial relationships and inexact graph matching},
}

@INPROCEEDINGS{9188213,
  ABSTRACT = {Hardware-Software Co-Design is a highly successful strategy for improving performance of domain-specific computing systems. We argue for the application of the same methodology to deep learning; specifically, we propose to extend neural architecture search with information about the hardware to ensure that the model designs produced are highly efficient in addition to the typical criteria around accuracy. Using the task of keyword spotting in audio on edge computing devices, we demonstrate that our approach results in neural architecture that is not only highly accurate, but also efficiently mapped to the computing platform which will perform the inference. Using our modified neural architecture search, we demonstrate 0.88\% increase in TOP-1 accuracy with 1.85× reduction in latency for keyword spotting in audio on an embedded SoC, and 1.59× on a high-end GPU.},
  AUTHOR = {{Anderson}, A. and {Su}, J. and {Dahyot}, R. and {Gregg}, D.},
  URL = {https://arxiv.org/pdf/2001.02976.pdf},
  BOOKTITLE = {2019 International Conference on High Performance Computing Simulation (HPCS)},
  DATE = {2019},
  DOI = {10.1109/HPCS48598.2019.9188213},
  EPRINT = {2001.02976},
  EPRINTTYPE = {arXiv},
  PAGES = {177--184},
  TITLE = {Performance-Oriented Neural Architecture Search},
}

@INPROCEEDINGS{DBLP:journals/corr/abs-1905-12678,
  ABSTRACT = {Grogan et al have recently proposed a solution to colour transfer by minimising the Euclidean distance L2 between two probability density functions capturing the colour distributions of two images (palette and target). It was shown to be very competitive to alternative solutions based on Optimal Transport for colour transfer. We show that in fact Grogan et al's formulation can also be understood as a new robust Optimal Transport based framework with entropy regularisation over marginals.},
  AUTHOR = {Dahyot, Rozenn and Alghamdi, Hana and Grogan, Mairéad},
  URL = {https://arxiv.org/pdf/1905.12678.pdf},
  BOOKTITLE = {Irish Machine Vision and Image Processing conference 2019},
  DATE = {2019},
  DOI = {10.21427/w611-mb37},
  EPRINT = {1905.12678},
  EPRINTTYPE = {arXiv},
  TITLE = {Entropic Regularisation of Robust Optimal Transport},
}

@INPROCEEDINGS{8902831,
  ABSTRACT = {Convolutional neural networks (CNNs) are very popular nowadays for image processing. CNNs allow one to learn optimal filters in a (mostly) supervised machine learning context. However this typically requires abundant labelled training data to estimate the filter parameters. Alternative strategies have been deployed for reducing the number of parameters and / or filters to be learned and thus decrease overfitting. In the context of reverting to preset filters, we propose here a computationally efficient harmonic block that uses Discrete Cosine Transform (DCT) filters in CNNs. In this work we examine the performance of harmonic networks in limited training data scenario. We validate experimentally that its performance compares well against scattering networks that use wavelets as preset filters.},
  AUTHOR = {{Ulicny}, M. and {Krylov}, V. A. and {Dahyot}, R.},
  URL = {https://mural.maynoothuniversity.ie/15158/1/RD_harmonic%20networks.pdf},
  BOOKTITLE = {2019 27th European Signal Processing Conference (EUSIPCO)},
  DATE = {2019-09},
  DOI = {10.23919/EUSIPCO.2019.8902831},
  EPRINT = {1905.00135},
  EPRINTTYPE = {arXiv},
  ISSN = {2219-5491},
  KEYWORDS = {Lapped Discrete Cosine Transform;harmonic network;convolutional filter;limited data},
  NOTE = {Github: https://github.com/matej-ulicny/harmonic-networks and paper also on arxiv http://arxiv.org/abs/1905.00135 and https://www.eurasip.org/Proceedings/Eusipco/eusipco2019/Proceedings/papers/1570533913.pdf},
  PAGES = {1--5},
  TITLE = {Harmonic Networks with Limited Training Samples},
}

@INPROCEEDINGS{DBLP:journals/corr/abs-1901-05049,
  ABSTRACT = {Next generation of embedded Information and Communication Technology (ICT) systems are interconnected collaborative intelligent systems able to perform autonomous tasks. Training and deployment of such systems on Edge devices however require a fine-grained integration of data and tools to achieve high accuracy and overcome functional and non-functional requirements. In this work, we present a modular AI pipeline as an integrating framework to bring data, algorithms and deployment tools together. By these means, we are able to interconnect the different entities or stages of particular systems and provide an end-to-end development of AI products. We demonstrate the effectiveness of the AI pipeline by solving an Automatic Speech Recognition challenge and we show that all the steps leading to an end-to-end development for Key-word Spotting tasks: importing, partitioning and pre-processing of speech data, training of different neural network architectures and their deployment on heterogeneous embedded platforms.},
  AUTHOR = {de Prado, Miguel and Su, Jing and Dahyot, Rozenn and Saeed, Rabia and Keller, Lorenzo and Vállez, Noelia},
  URL = {https://arxiv.org/pdf/1901.05049v1.pdf},
  BOOKTITLE = {HiPEAC 2019 workshop Emerging Deep Learning Accelerator},
  DATE = {2019},
  EPRINT = {1901.05049v1},
  EPRINTTYPE = {arXiv},
  TITLE = {{AI} Pipeline - bringing {AI} to you. End-to-end integration of data, algorithms and deployment tools},
}

@ARTICLE{AHMAD2019110,
  ABSTRACT = {This paper addresses the problem of floods classification and floods aftermath detection based on both social media and satellite imagery. Automatic detection of disasters such as floods is still a very challenging task. The focus lies on identifying passable routes or roads during floods. Two novel solutions are presented, which were developed for two corresponding tasks at the MediaEval 2018 benchmarking challenge. The tasks are (i) identification of images providing evidence for road passability and (ii) differentiation and detection of passable and non-passable roads in images from two complementary sources of information. For the first challenge, we mainly rely on object and scene-level features extracted through multiple deep models pre-trained on the ImageNet and Places datasets. The object and scene-level features are then combined using early, late and double fusion techniques. To identify whether or not it is possible for a vehicle to pass a road in satellite images, we rely on Convolutional Neural Networks and a transfer learning-based classification approach. The evaluation of the proposed methods is carried out on the large-scale datasets provided for the benchmark competition. The results demonstrate significant improvement in the performance over the recent state-of-art approaches.},
  AUTHOR = {Ahmad, Kashif and Pogorelov, Konstantin and Riegler, Michael and Ostroukhova, Olga and Halvorsen, Pål and Conci, Nicola and Dahyot, Rozenn},
  URL = {https://mural.maynoothuniversity.ie/15100/1/RD_signal.pdf},
  DATE = {2019},
  DOI = {10.1016/j.image.2019.02.002},
  ISSN = {0923-5965},
  JOURNALTITLE = {Signal Processing: Image Communication},
  KEYWORDS = {Flood detection,Convolutional neural networks,Natural disasters,Social media,Satellite imagery,Multimedia indexing and retrieval},
  NOTE = {Arxiv: https://arxiv.org/pdf/1901.03298.pdf},
  PAGES = {110--118},
  TITLE = {Automatic detection of passable roads after floods in remote sensed and social media data},
  VOLUME = {74},
}

@INPROCEEDINGS{IMVIP2019Albluwi,
  ABSTRACT = {Noise reduction algorithms have often been evaluated using images degraded by artificially synthesised noise. The RENOIR image dataset provides an alternative way for testing noise reduction algorithms on real noisy images and we propose in this paper to assess our CNN called De-Blurring Super-Resolution (DBSR) to reduce the natural noise due to low light conditions in a RENOIR dataset.},
  AUTHOR = {Albluwi, Fatma and Krylov, Vladimir A. and Dahyot, R.},
  LOCATION = {Technological University Dublin},
  URL = {https://arrow.tudublin.ie/cgi/viewcontent.cgi?article = 1006&context = impstwo},
  BOOKTITLE = {Irish Machine Vision and Image Processing (IMVIP 2019)},
  DATE = {2019},
  DOI = {10.21427/g34k-8r27},
  PAGES = {76--79},
  TITLE = {Denoising RENOIR Image Dataset with DBSR},
}

@INPROCEEDINGS{BMVC2019,
  ABSTRACT = {Convolutional neural networks (CNNs) learn filters in order to capture local correlation patterns in feature space. In contrast, in this paper we propose harmonic blocks that produce features by learning optimal combinations of responses to preset spectral filters. We rely on the use of the Discrete Cosine Transform filters which have excellent energy compaction properties and are widely used for image compression. The proposed harmonic blocks are intended to replace conventional convolutional layers to produce partially or fully harmonic versions of new or existing CNN architectures. We demonstrate how the harmonic networks can be efficiently compressed by exploiting redundancy in spectral domain and truncating high-frequency information. We extensively validate our approach and show that the introduction of harmonic blocks into state-of-the-art CNN models results in improved classification performance on CIFAR and ImageNet datasets.},
  AUTHOR = {Ulicny, M. and Krylov, V. and Dahyot, R.},
  LOCATION = {Cardiff UK},
  URL = {https://bmvc2019.org/wp-content/uploads/papers/0628-paper.pdf},
  BOOKTITLE = {British Machine Vision Conference (BMVC)},
  DATE = {2019},
  NOTE = {Github: https://github.com/matej-ulicny/harmonic-networks},
  TITLE = {Harmonic Networks for Image Classification},
}

@ARTICLE{GROGAN2019,
  ABSTRACT = {Optimal Transport (OT) is a very popular framework for performing colour transfer in images and videos. We have proposed an alternative framework where the cost function used for inferring a parametric transfer function is defined as the robust L2 divergence between two probability density functions. In this paper, we show that our approach combines many advantages of state of the art techniques and outperforms many recent algorithms as measured quantitatively with standard quality metrics, and qualitatively using perceptual studies. Mathematically, our formulation is presented in contrast to the OT cost function that shares similarities with our cost function. Our formulation, however, is more flexible as it allows colour correspondences that may be available to be taken into account and performs well despite potential occurrences of correspondence outlier pairs. Our algorithm is shown to be fast, robust and it easily allows for user interaction providing freedom for artists to fine tune the recoloured images and videos.},
  AUTHOR = {Grogan, Mairead and Dahyot, Rozenn},
  URL = {https://mural.maynoothuniversity.ie/15103/1/RB_L2.pdf},
  DATE = {2019},
  DOI = {10.1016/j.cviu.2019.02.002},
  ISSN = {1077-3142},
  JOURNALTITLE = {Computer Vision and Image Understanding},
  KEYWORDS = {Colour Transfer,L2 Registration,Re-colouring,Colour Grading},
  NOTE = {Github: https://github.com/groganma/gmm-colour-transfer},
  TITLE = {L2 Divergence for robust colour transfer},
}

@INPROCEEDINGS{8904931,
  ABSTRACT = {This study investigates the visual place retrieval of an image query using a geotagged image dataset. Vector of Locally Aggregated Descriptors (VLAD) is one of the local features that can be used for image place recognition. VLAD describes an image by the difference of its local feature descriptors from an already computed codebook. Generally, a visual codebook is generated from k-means clustering of the descriptors. However, the dimensionality of visual features is not trivial and the computational load of sample distances in a large image dataset is challenging. In order to design an accurate image retrieval method with affordable computation expenses, we propose to use the mini-batch k-means clustering to compute VLAD descriptor(MB-VLAD). The proposed MBVLAD technique shows advantage in retrieval accuracy in comparison with the state of the art techniques.},
  AUTHOR = {{Aljuaidi}, R. and {Su}, J. and {Dahyot}, R.},
  URL = {https://mural.maynoothuniversity.ie/15129/1/RD_mini%20batch.pdf},
  BOOKTITLE = {2019 30th Irish Signals and Systems Conference (ISSC)},
  DATE = {2019-06},
  DOI = {10.1109/ISSC.2019.8904931},
  ISSN = {2688-1446},
  KEYWORDS = {feature extraction;content-based image retrieval;image processing},
  NOTE = {Awarded Best Student Paper at ISSC 2019. Github: https://github.com/ReemTCD/Mini_Batch_VLAD},
  PAGES = {1--6},
  TITLE = {Mini-Batch VLAD for Visual Place Retrieval},
}

@INPROCEEDINGS{10.1007/978-3-030-13453-2_7,
  ABSTRACT = {We explore the applicability and limitations of a state-of-the-art object detection and geotagging system [4] applied to crowdsourced image data. Our experiments with imagery from Mapillary crowdsourcing platform demonstrate that with increasing amount of images, the detection accuracy is getting close to that obtained with high-end street level data. Nevertheless, due to excessive camera position noise, the estimated geolocation (position) of the detected object is less accurate on crowdsourced Mapillary imagery than with high-end street level imagery obtained by Google Street View.},
  AUTHOR = {Krylov, Vladimir A. and Dahyot, Rozenn},
  EDITOR = {Alzate, Carlos and Monreale, Anna and Assem, Haytham and Bifet, Albert and Buda, Teodora Sandra and Caglayan, Bora and Drury, Brett and García-Martín, Eva and Gavaldà, Ricard and Kramer, Stefan and Lavesson, Niklas and Madden, Michael and Molloy, Ian and Nicolae, Maria-Irina and Sinn, Mathieu},
  LOCATION = {Cham},
  PUBLISHER = {Springer International Publishing},
  URL = {https://mural.maynoothuniversity.ie/15249/1/RD_object.pdf},
  BOOKTITLE = {ECML PKDD 2018 Workshops},
  DATE = {2019},
  DOI = {10.1007/978-3-030-13453-2_7},
  ISBN = {978-3-030-13453-2},
  PAGES = {79--83},
  TITLE = {Object Geolocation from Crowdsourced Street Level Imagery},
}

@INPROCEEDINGS{8902611,
  ABSTRACT = {This paper proposes a new colour transfer method with Optimal transport to transfer the colour of a source image to match the colour of a target image of the same scene. We propose to formulate the problem in higher dimensional spaces (than colour spaces) by encoding overlapping neighborhoods of pixels containing colour information as well as spatial information. Since several recoloured candidates are now generated for each pixel in the source image, we define an original procedure to efficiently merge these candidates which allows denoising and artifact removal as well as colour transfer. Experiments show quantitative and qualitative improvements over previous colour transfer methods. Our method can be applied to different contexts of colour transfer such as transferring colour between different camera models, camera settings, illumination conditions and colour retouch styles for photographs.},
  AUTHOR = {{Alghamdi}, H. and {Grogan}, M. and {Dahyot}, R.},
  URL = {https://www.eurasip.org/Proceedings/Eusipco/eusipco2019/Proceedings/papers/1570533179.pdf},
  BOOKTITLE = {2019 27th European Signal Processing Conference (EUSIPCO)},
  DATE = {2019-09},
  DOI = {10.23919/EUSIPCO.2019.8902611},
  ISSN = {2219-5491},
  KEYWORDS = {optimal transport;colour transfer;image enhancement;JPEG compression blocks},
  NOTE = {Github: https://github.com/leshep/PCT_OT},
  PAGES = {1--5},
  TITLE = {Patch-Based Colour Transfer with Optimal Transport},
}

@INPROCEEDINGS{8903000,
  ABSTRACT = {Single Image Super-Resolution (SISR) has witnessed a dramatic improvement in recent years through the use of deep learning and, in particular, convolutional neural networks (CNN). In this work we address reconstruction from low-resolution images and consider as well degrading factors in images such as blurring. To address this challenging problem, we propose a new architecture to tackle blur with the down-sampling of images by extending the DBSRCNN architecture. We validate our new architecture (DBSR) experimentally against several state of the art super-resolution techniques.},
  AUTHOR = {{Albluwi}, F. and {Krylov}, V. A. and {Dahyot}, R.},
  URL = {https://www.eurasip.org/Proceedings/Eusipco/eusipco2019/Proceedings/papers/1570533420.pdf},
  BOOKTITLE = {2019 27th European Signal Processing Conference (EUSIPCO)},
  DATE = {2019-09},
  DOI = {10.23919/EUSIPCO.2019.8903000},
  ISSN = {2219-5491},
  KEYWORDS = {Image super-resolution;image deblurring;deep learning;CNN},
  NOTE = {Github: https://github.com/Fatma-Albluwi/DBSR},
  PAGES = {1--5},
  TITLE = {Super-Resolution on Degraded Low-Resolution Images Using Convolutional Neural Networks},
}

@INPROCEEDINGS{Bhatia2019,
  ABSTRACT = {This paper investigates data synthesis with a Generative Adversarial Network (GAN) for augmenting the amount of data used for training classifiers (in supervised learning) to compensate for class imbalance (when the classes are not represented equally by the same number of training samples). Our data synthesis approach with GAN is compared with data augmentation in the context of image classification. Our experimental results show encouraging results in comparison to standard data augmentation schemes based on image transforms.},
  AUTHOR = {Bhatia, S. and Dahyot, R.},
  EDITOR = {Curry, Edward and Keane, Mark and Ojo, Adegboyega and Salwala, Dhaval},
  LOCATION = {Galway, Ireland},
  URL = {http://ceur-ws.org/Vol-2563/aics_34.pdf},
  BOOKTITLE = {27th Irish Conference on Artificial Intelligence and Cognitive Science},
  DATE = {2019},
  ISSN = {1613-0073},
  PAGES = {365--375},
  TITLE = {Using WGAN for Improving Imbalanced Classification Performance},
}

@ARTICLE{Krylov_2018,
  ABSTRACT = {Many applications such as autonomous navigation, urban planning and asset monitoring, rely on the availability of accurate information about objects and their geolocations. In this paper we propose to automatically detect and compute the GPS coordinates of recurring stationary objects of interest using street view imagery. Our processing pipeline relies on two fully convolutional neural networks: the first segments objects in the images while the second estimates their distance from the camera. To geolocate all the detected objects coherently we propose a novel custom Markov Random Field model to perform objects triangulation. The novelty of the resulting pipeline is the combined use of monocular depth estimation and triangulation to enable automatic mapping of complex scenes with multiple visually similar objects of interest. We validate experimentally the effectiveness of our approach on two object classes: traffic lights and telegraph poles. The experiments report high object recall rates and GPS accuracy within 2 meters, which is comparable with the precision of single-frequency GPS receivers.},
  AUTHOR = {Krylov, Vladimir and Kenny, Eamonn and Dahyot, Rozenn},
  PUBLISHER = {{MDPI} {AG}},
  URL = {https://www.mdpi.com/2072-4292/10/5/661/pdf?version=1525349637},
  DATE = {2018-04},
  DOI = {10.3390/rs10050661},
  JOURNALTITLE = {Remote Sensing},
  NOTE = {Github: https://github.com/vlkryl/streetview_objectmapping - URI: http://hdl.handle.net/2262/89654 - Arxiv: https://arxiv.org/pdf/1708.08417.pdf},
  NUMBER = {5},
  PAGES = {661},
  TITLE = {Automatic Discovery and Geotagging of Objects from Street View Imagery},
  VOLUME = {10},
}

@INPROCEEDINGS{LiuIMVIP2018,
  AUTHOR = {Liu, C.-J. and Vladimir, K. and Dahyot, R.},
  LOCATION = {Ulster University, Northern Ireland},
  URL = {https://arxiv.org/pdf/2108.06306.pdf},
  BOOKTITLE = {Irish Machine Vision and Image Processing conference (IMVIP 2018)},
  DATE = {2018},
  NOTE = {http://hdl.handle.net/2262/89508},
  TITLE = {3D point cloud segmentation using GIS},
}

@REPORT{DBLP:journals/corr/abs-1812-03205,
  ABSTRACT = {Convolutional neural networks (CNNs) learn filters in order to capture local correlation patterns in feature space. In contrast, in this paper we propose harmonic blocks that produce features by learning optimal combinations of spectral filters defined by the Discrete Cosine Transform. The harmonic blocks are used to replace conventional convolutional layers to construct partial or fully harmonic CNNs. We extensively validate our approach and show that the introduction of harmonic blocks into state-of-the-art CNN baseline architectures results in comparable or better performance in classification tasks on small NORB, CIFAR10 and CIFAR100 datasets.},
  AUTHOR = {Ulicny, Matej and Krylov, Vladimir A. and Dahyot, Rozenn},
  INSTITUTION = {Trinity College Dublin Ireland},
  URL = {https://arxiv.org/pdf/1812.03205.pdf},
  DATE = {2018},
  EPRINT = {1812.03205},
  EPRINTTYPE = {arXiv},
  NOTE = {Github: https://github.com/matej-ulicny/harmonic-networks},
  TITLE = {Harmonic Networks: Integrating Spectral Information into CNNs},
  TYPE = {techreport},
}

@INPROCEEDINGS{8516983,
  ABSTRACT = {Recently multiple high performance algorithms have been developed to infer high-resolution images from low-resolution image input using deep learning algorithms. The related problem of super-resolution from blurred or corrupted lowresolution images has however received much less attention. In this work, we propose a new deep learning approach that simultaneously addresses deblurring and super-resolution from blurred low resolution images. We evaluate the stateof-the-art super-resolution convolutional neural network (SRCNN) architecture proposed in [1] for the blurred reconstruction scenario and propose a revised deeper architecture that proves its superiority experimentally both when the levels of blur are known and unknown a priori.},
  AUTHOR = {Albluwi, F. and Krylov, V. A. and Dahyot, R.},
  URL = {https://mural.maynoothuniversity.ie/15254/1/RD_image.pdf},
  BOOKTITLE = {2018 IEEE 28th International Workshop on Machine Learning for Signal Processing (MLSP)},
  DATE = {2018-09},
  DOI = {10.1109/MLSP.2018.8516983},
  ISSN = {1551-2541},
  KEYWORDS = {convolution;image reconstruction;image resolution;image restoration;learning (artificial intelligence);neural nets;image deblurring;deep convolutional neural networks;multiple high performance algorithms;high-resolution images;low-resolution image input;deep learning algorithms;low-resolution images;deep learning approach;blurred low resolution images;super-resolution convolutional neural network;Training;Image resolution;Pipelines;Image reconstruction;Signal resolution;Feature extraction;Convolutional neural networks;Image super-resolution;deblurring;deep learning;convolutional neural networks},
  NOTE = {Github: https://github.com/Fatma-Albluwi/DBSRCNN},
  PAGES = {1--6},
  TITLE = {Image Deblurring and Super-Resolution Using Deep Convolutional Neural Networks},
}

@INPROCEEDINGS{8451458,
  ABSTRACT = {Abundant image and sensory data collected over the last decades represents an invaluable source of information for cataloging and monitoring of the environment. Fusion of heterogeneous data sources is a challenging but promising tool to efficiently leverage such information. In this work we propose a pipeline for automatic detection and geolocation of recurring stationary objects deployed on fusion scenario of street level imagery and LiDAR point cloud data. The objects are geolocated coherently using a fusion procedure formalized as a Markov random field problem. This allows us to efficiently combine information from object segmentation, triangulation, monocular depth estimation and position matching with LiDAR data. The proposed fusion approach produces object mappings robust to scenes reporting multiple object instances. We introduce a new challenging dataset of over 200 traffic lights in Dublin city centre and demonstrate high performance of the proposed methodology and its capacity to perform multi-sensor data fusion.},
  AUTHOR = {Krylov, V. A. and Dahyot, R.},
  URL = {https://mural.maynoothuniversity.ie/15253/1/RD_object%20geolocation.pdf},
  BOOKTITLE = {2018 25th IEEE International Conference on Image Processing (ICIP)},
  DATE = {2018-10},
  DOI = {10.1109/ICIP.2018.8451458},
  ISSN = {2381-8549},
  KEYWORDS = {Laser radar;Three-dimensional displays;Cameras;Geology;Roads;Pipelines;Image segmentation;Object geolocation;street level imagery;LiDAR data;Markov random fields;traffic lights},
  PAGES = {2745--2749},
  TITLE = {Object Geolocation Using MRF Based Multi-Sensor Fusion},
}

@ARTICLE{GROGAN2018452,
  ABSTRACT = {We propose several cost functions for registration of shapes encoded with Euclidean and/or non-Euclidean information (unit vectors). Our framework is assessed for estimation of both rigid and non-rigid transformations between the target and model shapes corresponding to 2D contours and 3D surfaces. The experimental results obtained confirm that using the combination of a point's position and unit normal vector in a cost function can enhance the registration results compared to state of the art methods.},
  AUTHOR = {Grogan, Mairead and Dahyot, Rozenn},
  URL = {https://arxiv.org/pdf/1708.07791.pdf},
  DATE = {2018},
  DOI = {10.1016/j.patcog.2018.02.021},
  ISSN = {0031-3203},
  JOURNALTITLE = {Pattern Recognition},
  KEYWORDS = {Shape registration,Directional information,Von Mises-Fisher,registration},
  PAGES = {452--466},
  TITLE = {Shape registration with directional data},
  VOLUME = {79},
}

@INPROCEEDINGS{Llewellynn:2017:BPO:3075564.3076259,
  ABSTRACT = {The Bonseyes EU H2020 collaborative project aims to develop a platform consisting of a Data Marketplace, a Deep Learning Toolbox, and Developer Reference Platforms for organizations wanting to adopt Artificial Intelligence. The project will be focused on using artificial intelligence in low power Internet of Things (IoT) devices ("edge computing"), embedded computing systems, and data center servers ("cloud computing"). It will bring about orders of magnitude improvements in efficiency, performance, reliability, security, and productivity in the design and programming of systems of artificial intelligence that incorporate Smart Cyber-Physical Systems (CPS). In addition, it will solve a causality problem for organizations who lack access to Data and Models. Its open software architecture will facilitate adoption of the whole concept on a wider scale. To evaluate the effectiveness, technical feasibility, and to quantify the real-world improvements in efficiency, security, performance, effort and cost of adding AI to products and services using the Bonseyes platform, four complementary demonstrators will be built. Bonseyes platform capabilities are aimed at being aligned with the European FI-PPP activities and take advantage of its flagship project FIWARE. This paper provides a description of the project motivation, goals and preliminary work.},
  AUTHOR = {Llewellynn, Tim and Fernández-Carrobles, M. Milagro and Deniz, Oscar and Fricker, Samuel and Storkey, Amos and Pazos, Nuria and Velikic, Gordana and Leufgen, Kirsten and Dahyot, Rozenn and Koller, Sebastian and Goumas, Georgios and Leitner, Peter and Dasika, Ganesh and Wang, Lei and Tutschku, Kurt},
  LOCATION = {Siena, Italy},
  PUBLISHER = {ACM},
  URL = {http://dl.acm.org/ft_gateway.cfm?id=3076259&type=pdf},
  BOOKTITLE = {Proceedings of the Computing Frontiers Conference},
  DATE = {2017},
  DOI = {10.1145/3075564.3076259},
  ISBN = {978-1-4503-4487-6},
  KEYWORDS = {Data marketplace,Deep Learning,Internet of things,Smart Cyber-Physical Systems},
  PAGES = {299--304},
  SERIES = {CF'17},
  TITLE = {BONSEYES: Platform for Open Development of Systems of Artificial Intelligence: Invited Paper},
}

@INPROCEEDINGS{MatejIMVIP2017,
  ABSTRACT = {This paper investigates the use of Convolutional Neural Networks (CNN) to classify images encoded in compressible form using Discrete Cosine Tranform (DCT) as an alternative to raw image format. We show experimentally that DCT features, that are directly available from JPEG format for instance, can be processed as efficiently as raw image data using the same CNN architectures.},
  AUTHOR = {Ulicny, M. and Dahyot, R.},
  LOCATION = {Maynooth University},
  URL = {http://mural.maynoothuniversity.ie/8841/1/IMVIP2017_Proceedings.pdf},
  BOOKTITLE = {Irish Machine Vision and Image Processing conference (IMVIP 2017)},
  DATE = {2017},
  NOTE = {http://mural.maynoothuniversity.ie/8841/},
  PAGES = {44--51},
  TITLE = {On Using CNN with Compressed (DCT Based) Image Data},
}

@INPROCEEDINGS{ImanIMVIP2017,
  ABSTRACT = {This paper introduces an automatic procedure for aligning and stitching the medical images of skin scars that have the various amount of overlapping into one single registered image. The alignment procedure is based on the rigid transformation of the pair of images regarding detected matched features. The proposed paper compares four different feature detection methods and evaluates the methods on several clinical cases. For each case, the initial image is divided into four smaller sub-images with the different dimension. The result shows that the Harris Corner Detector algorithm achieves nearly 99\% accurate result with the minimum overlapping of 160 pixels as the fastest method.},
  AUTHOR = {Zolanvari, S. M. I. and Dahyot, R.},
  LOCATION = {Maynooth University},
  URL = {http://mural.maynoothuniversity.ie/8841/1/IMVIP2017_Proceedings.pdf},
  BOOKTITLE = {Irish Machine Vision and Image Processing conference (IMVIP 2017)},
  DATE = {2017},
  NOTE = {http://mural.maynoothuniversity.ie/8841/},
  PAGES = {265--268},
  TITLE = {Stitching Skin Images of Scars},
}

@INPROCEEDINGS{HanaIMVIP2017,
  ABSTRACT = {Registration techniques have many applications such as 3D scans alignment, panoramic image mosaic creation or shape matching. This paper focuses on (2D) point cloud registration using novel iterative algorithms that are inspired by the Iterative Distribution Transfer (IDT) algorithm originally proposed to solve colour transfer [Pitié et al., 2005, Pitié et al., 2007]. We propose three variants to IDT algorithm that we compare with the standard L2 shape registration technique [Jian and Vemuri, 2011]. We show that our IDT algorithms perform well against L2 for finding correspondences between model and target shapes.},
  AUTHOR = {Alghamdi, H. and Grogan, M. and Dahyot, R.},
  LOCATION = {Maynooth University},
  URL = {http://mural.maynoothuniversity.ie/8841/1/IMVIP2017_Proceedings.pdf},
  BOOKTITLE = {Irish Machine Vision and Image Processing conference (IMVIP 2017)},
  DATE = {2017},
  NOTE = {http://mural.maynoothuniversity.ie/8841/},
  PAGES = {91--98},
  TITLE = {IDT Vs L2 Distance for Point Set Registration},
}

@ARTICLE{BulbulCAVW2016,
  ABSTRACT = {We propose to automatically populate geo‐located virtual cities by harvesting and analyzing online contents shared on social networks and websites. We show how pose and motion paths of agents can be realistically rendered using information gathered from social media. 3D cities are automatically generated using open‐source information available online. To provide our final rendering of both static and dynamic urban scenes, we use Unreal game engine.},
  AUTHOR = {Bulbul, Abdullah and Dahyot, Rozenn},
  URL = {https://roznn.github.io/PDF/Bulbul_CAVW2017.pdf},
  DATE = {2017},
  DOI = {10.1002/cav.1742},
  EPRINT = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cav.1742},
  JOURNALTITLE = {Computer Animation and Virtual Worlds},
  KEYWORDS = {computer animation,crowd simulations,social media,virtual worlds},
  NUMBER = {5},
  PAGES = {e1742},
  TITLE = {Populating virtual cities using social media},
  VOLUME = {28},
}

@REPORT{DBLP:journals/corr/GroganD17,
  ABSTRACT = {We present a flexible approach to colour transfer inspired by techniques recently proposed for shape registration. Colour distributions of the palette and target images are modelled with Gaussian Mixture Models (GMMs) that are robustly registered to infer a non linear parametric transfer function. We show experimentally that our approach compares well to current techniques both quantitatively and qualitatively. Moreover, our technique is computationally the fastest and can take efficient advantage of parallel processing architectures for recolouring images and videos. Our transfer function is parametric and hence can be stored in memory for later usage and also combined with other computed transfer functions to create interesting visual effects. Overall this paper provides a fast user friendly approach to recolouring of image and video materials.},
  AUTHOR = {Grogan, Mairéad and Dahyot, Rozenn},
  INSTITUTION = {Trinity College Dublin Ireland},
  URL = {https://arxiv.org/pdf/1705.06091.pdf},
  DATE = {2017},
  EPRINT = {1705.06091},
  EPRINTTYPE = {arXiv},
  TITLE = {Robust Registration of Gaussian Mixtures for Colour Transfer},
  TYPE = {techreport},
}

@ARTICLE{BulbulCAG2016,
  ABSTRACT = {This paper proposes to use a geotagged virtual world for the visualization of people’s visual interest and their sentiment as captured from their social network activities. Using mobile devices, people widely share their experiences and the things they find interesting through social networks. We experimentally show that accumulating information over a period of time from multiple social network users allows to efficiently map and visualize popular landmarks as found in cities such as Rome in Italy and Dublin in Ireland. The proposed approach is also sensitive to temporal and spatial events that attract visual attention. We visualize the calculated popularity on 3D virtual cities using game engine technologies.},
  AUTHOR = {Bulbul, Abdullah and Dahyot, Rozenn},
  URL = {https://mural.maynoothuniversity.ie/15107/1/RD_social.pdf},
  DATE = {2017},
  DOI = {10.1016/j.cag.2017.01.005},
  ISSN = {0097-8493},
  JOURNALTITLE = {Computers \& Graphics},
  KEYWORDS = {3D cities},
  PAGES = {28--36},
  TITLE = {Social media based 3D visual popularity},
  VOLUME = {63},
}

@REPORT{Drone2017,
  AUTHOR = {Byrne, J. and Connelly, J. and Su, J. and Krylov, V. and Bourke, M. and Moloney, D. and Dahyot, R.},
  INSTITUTION = {School of Computer Science and Statistics, Trinity College Dublin},
  URL = {http://www.tara.tcd.ie/bitstream/handle/2262/81836/tcd3dintelmovidius2017-drone-imagery%5b2%5d.pdf},
  DATE = {2017},
  NOTE = {Tech report, mesh and Drone Images available URI: http://hdl.handle.net/2262/81836},
  TITLE = {Trinity College Dublin Drone Survey Dataset},
  TYPE = {techreport},
}

@INPROCEEDINGS{Grogan:2017:UII:3150165.3150171,
  ABSTRACT = {Recently, an example based colour transfer approach proposed modelling the colour distributions of a palette and target image using Gaussian Mixture Models, and registers them by minimising the robust £2 distance between the mixtures. In this paper we propose to extend this approach to allow for user interaction. We present two interactive recolouring applications, the first allowing the user to select colour correspondences between a target and palette image, while the second palette based application allows the user to edit a palette of colours to determine the image recolouring. We modify the £2 based cost function to improve results when an interactive interface is used, and take measures to ensure that even when minimal input is given by the user, good colour transfer results are created. Both applications are available through a web interface and qualitatively assessed against recent recolouring techniques.},
  AUTHOR = {Grogan, Mairéad and Dahyot, Rozenn and Smolic, Aljosa},
  LOCATION = {London, United Kingdom},
  PUBLISHER = {ACM},
  URL = {http://doi.acm.org/10.1145/3150165.3150171},
  BOOKTITLE = {Proceedings of the 14th European Conference on Visual Media Production (CVMP 2017)},
  DATE = {2017},
  DOI = {10.1145/3150165.3150171},
  ISBN = {978-1-4503-5329-8},
  KEYWORDS = {L2 Registration,Colour transfer,palette based image recoloring},
  NOTE = {Awarded best paper CVMP 2017},
  PAGES = {6:1--6:10},
  SERIES = {CVMP 2017},
  TITLE = {User Interaction for Image Recolouring Using L2},
}

@INPROCEEDINGS{DiECCV2016,
  ABSTRACT = {Despite strong progress in the field of 3D reconstruction from multiple views, holes on objects, transparency of objects and textureless scenes, continue to be open challenges. On the other hand, silhouette based reconstruction techniques ease the dependency of 3d reconstruction on image pixels but need a large number of silhouettes to be available from multiple views. In this paper, a novel end to end pipeline is proposed to produce high quality reconstruction from a low number of silhouettes, the core of which is a deep shape reconstruction architecture. Evaluations on ShapeNet [1] show good quality of reconstruction compared with ground truth.},
  AUTHOR = {Di, Xinhan and Dahyot, Rozenn and Prasad, Mukta},
  EDITOR = {Hua, Gang and Jégou, Hervé},
  LOCATION = {Cham},
  PUBLISHER = {Springer International Publishing},
  URL = {https://mural.maynoothuniversity.ie/15258/1/RD_deep%20shape.pdf},
  BOOKTITLE = {Computer Vision -- ECCV 2016 Workshops},
  DATE = {2016},
  DOI = {10.1007/978-3-319-49409-8},
  ISBN = {978-3-319-49409-8},
  PAGES = {251--265},
  TITLE = {Deep Shape from a Low Number of Silhouettes},
}

@INPROCEEDINGS{GroganIMVIP2016,
  ABSTRACT = {This paper investigates how several techniques can be used together for colouring frames in grey level sequences. A trained deep neural network is used to colour a grey level image coherently , and this colour image can be recoloured further to change its feel. When considering videos however, artifacts are created in the first step when the same semantic object can occasionally be given different colours from frame to frame in the sequence creating a flicker in the resulting coloured sequence.},
  AUTHOR = {Grogan, M. and Carvalho, J. and Dahyot, R.},
  LOCATION = {Galway, Ireland},
  URL = {https://aran.library.nuigalway.ie/bitstream/handle/10379/6136/IMVIP2016Book.pdf},
  BOOKTITLE = {Irish Machine Vision and Image Processing Conference (IMVIP 2016)},
  DATE = {2016-08},
  KEYWORDS = {colour transfer,colouring,deep learning,flicker},
  NOTE = {URI http://hdl.handle.net/10379/6136},
  TITLE = {Recent techniques for (re)colouring},
}

@ARTICLE{ArellanoPR2015,
  ABSTRACT = {The Euclidian distance between Gaussian Mixtures has been shown to be robust to perform point set registration (Jian and Vemuri, 2011). We propose to extend this idea for robustly matching a family of shapes (ellipses). Optimisation is performed with an annealing strategy, and the search for occurrences is repeated several times to detect multiple instances of the shape of interest. We compare experimentally our approach to other state-of-the-art techniques on a benchmark database for ellipses, and demonstrate the good performance of our approach.},
  AUTHOR = {Arellano, Claudia and Dahyot, Rozenn},
  URL = {https://mural.maynoothuniversity.ie/15108/1/D_robust.pdf},
  DATE = {2016},
  DOI = {10.1016/j.patcog.2016.01.017},
  ISSN = {0031-3203},
  JOURNALTITLE = {Pattern Recognition},
  KEYWORDS = {Ellipse detection,L2 distance,GMM,Parameter estimation},
  NOTE = {Github https://github.com/clarella/L2-Ellipse-Fitting},
  PAGES = {12--26},
  TITLE = {Robust ellipse detection with Gaussian mixture models},
  VOLUME = {58},
}

@INPROCEEDINGS{BulbulIMVIP2015,
  ABSTRACT = {Despite the recent advances in 3D reconstruction from images, the state of the art methods fail to ac- curately reconstruct objects with reflective materials. The underlying reason for this inaccuracy is that the detected image features belong to the reflected scene instead of the reconstructed object and do not lie on the surface of the object. In this study, we propose a method to refine the 3D reconstruction of reflective convex surfaces. This method utilizes the geometrical distortion of the reflected scenes behind a spherical surface.},
  AUTHOR = {Bulbul, A. and Grogan, M. and Dahyot, R.},
  LOCATION = {Dublin, Ireland},
  URL = {http://www.tara.tcd.ie/bitstream/handle/2262/74714/IMVIP2015Book.pdf},
  BOOKTITLE = {Irish Machine Vision and Image Processing Conference (IMVIP 2015)},
  DATE = {2015-08},
  KEYWORDS = {3D reconstruction,Shape from images,Hough Transform,Specular surface},
  NOTE = {URI http://hdl.handle.net/2262/74714},
  PAGES = {19--26},
  TITLE = {3D Reconstruction of Reflective Spherical Surfaces from Multiple Images},
}

@INPROCEEDINGS{XiaIMVIP2015,
  ABSTRACT = {Hand hygiene is the most effective way in preventing the health care-associated infection. In this work, we propose to investigate the automatic recognition of the hand hygiene poses with RGB-D videos. Different classifiers are experimented with the Histogram of Oriented Gradient (HOG) features extracted from the hand regions. With a frame-level classification rate of more than 95\%, and with 100\% video-level classification rate, we demonstrate the effectiveness of our method for recognizing these hand hygiene poses. Also, we demonstrate that using the temporal information, and combining the color with depth information can improve the recognition accuracy.},
  AUTHOR = {Xia, B. and Dahyot, R. and Ruttle, J. and Caulfield, D. and Lacey, G.},
  LOCATION = {Dublin, Ireland},
  URL = {http://www.tara.tcd.ie/bitstream/handle/2262/74714/IMVIP2015Book.pdf},
  BOOKTITLE = {Irish Machine Vision and Image Processing Conference (IMVIP 2015)},
  DATE = {2015-08},
  KEYWORDS = {Hand Hygiene,Poses Recognition,RGB-D},
  NOTE = {URI http://hdl.handle.net/2262/74714},
  PAGES = {43--50},
  TITLE = {Hand Hygiene Poses Recognition with RGB-D Videos},
}

@INPROCEEDINGS{DahyotIWCIM2015,
  ABSTRACT = {This paper tackles the audio visual renderings of geolocated datasets harvested from social networks. These datasets are noisy, multimodal and heterogeneous by nature, providing different fields of information. We focus here on the information of location (GPS), time (timestamp) and text from tweets from which sentiment is extracted. We provide two ways for visualising datasets and for which demos can be seen online.},
  AUTHOR = {Dahyot, R. and Brady, C. and Bourges, C. and Bulbul, A.},
  LOCATION = {Prague, Czech Republic},
  URL = {https://mural.maynoothuniversity.ie/15259/1/RD_information.pdf},
  BOOKTITLE = {2015 International Workshop on Computational Intelligence for Multimedia Understanding (IWCIM)},
  DATE = {2015},
  DOI = {10.1109/IWCIM.2015.7347082},
  KEYWORDS = {Global Positioning System;data visualisation;rendering (computer graphics);social networking (online);GPS;audio visual rendering;dataset visualization;geolocated datasets;information visualisation;location information;sentiment extraction;social media analytics;social networks;timestamp;Geology;Google;Heating;Media;Silicon;Visualization;Social Media Analytics;Visualisation},
  TITLE = {Information visualisation for social media analytics},
}

@INPROCEEDINGS{GroganEusipco2015,
  ABSTRACT = {This paper proposes to perform colour transfer by minimising a divergence (the L2 distance) between two colour distributions. We propose to model each dataset by a compact Gaussian mixture which is designed for the specific purpose of colour transfer between images which have different scene content. A non rigid transformation is estimated by minimising the Euclidean distance (L2) between these two distributions, and the estimated transformation is used for transferring colour statistics from one image to another. Experimental results show that this is a very promising approach for transferring colour and it performs very well against an alternative reference approach.},
  AUTHOR = {Grogan, M. and Prasad, M. and Dahyot, R.},
  LOCATION = {Nice France},
  URL = {https://mural.maynoothuniversity.ie/15261/1/RD_L2%20registration.pdf},
  BOOKTITLE = {European Signal Processing Conference (Eusipco)},
  DATE = {2015-09},
  DOI = {10.1109/EUSIPCO.2015.7362799},
  NOTE = {https://www.eurasip.org/Proceedings/Eusipco/Eusipco2015/papers/1570102575.pdf},
  TITLE = {L2 Registration for Colour Transfer},
}

@PROCEEDINGS{IMVIP2015,
  EDITOR = {Dahyot, Rozenn and Lacey, Gerard and Dawson-Howe, Kenneth and Pitie, Francois and Moloney, David},
  LOCATION = {Dublin, Ireland},
  PUBLISHER = {Irish Pattern Recognition and Classification Society (ISBN 978-0-9934207-0-2)},
  URL = {http://www.tara.tcd.ie/bitstream/handle/2262/74714/IMVIP2015Book.pdf},
  DATE = {2015-08},
  NOTE = {URI: http://hdl.handle.net/2262/74714},
  TITLE = {IRISH MACHINE VISION and IMAGE PROCESSING Conference proceedings 2015},
}

@INPROCEEDINGS{GroganCVMP2015,
  ABSTRACT = {We propose a method for colour transfer by minimising the L2 distance between two colour distributions. We use Gaussian Mixture Models (GMMs) to model the colour distribution of the target and palette images and use L2 to find a transformation φ which register the GMM's. The L2 distance has been shown to be robust for shape registration application [2]. The function φ is modelled as either an affine or Thin Plate Spline transformation controlled by a latent vector θ. The affine function consists of a 3x3 matrix A and 3D vector offset o.},
  AUTHOR = {Grogan, Mairead and Dahyot, Rozenn},
  LOCATION = {London, United Kingdom},
  PUBLISHER = {ACM},
  URL = {http://doi.acm.org/10.1145/2824840.2824862},
  BOOKTITLE = {Proceedings of the 12th European Conference on Visual Media Production},
  DATE = {2015},
  DOI = {10.1145/2824840.2824862},
  ISBN = {978-1-4503-3560-7},
  NOTE = {Awarded Best Student Poster at CVMP 2015},
  PAGES = {16:1--16:1},
  SERIES = {CVMP '15},
  TITLE = {L2 Registration for Colour Transfer in Videos},
}

@INPROCEEDINGS{BulbulCVMP2015,
  ABSTRACT = {Social Media is a very rich source of up-to-date localized information. In recent years, image collections from photo sharing websites (e.g. Flicker) have been effectively used for 3D reconstruction of objects, buildings and even cities. While 3D reconstruction techniques are highly improved in terms of accuracy, performance, and parallelism there are still means to utilize the up-to-date information available from public social sharing websites such as Twitter and Instagram for continuous refinement of the 3D models and information visualization. Our emphasis is on utilizing the information for detecting and refining the changes in the scene, adding new structures and visualizing saliency/popularity information in 3D.},
  AUTHOR = {Bulbul, Abdullah and Dahyot, Rozenn},
  LOCATION = {London, United Kingdom},
  PUBLISHER = {ACM},
  URL = {http://doi.acm.org/10.1145/2824840.2824860},
  BOOKTITLE = {Proceedings of the 12th European Conference on Visual Media Production},
  DATE = {2015},
  DOI = {10.1145/2824840.2824860},
  ISBN = {978-1-4503-3560-7},
  PAGES = {20:1--20:1},
  SERIES = {CVMP '15},
  TITLE = {Social Media Based 3D Modeling and Visualization},
}

@INPROCEEDINGS{graisearchIMVIP2014,
  AUTHOR = {Zdziarski, Z. and Mitchell, J. and Houdyer, P. and Johnson, D. and Bourges, C. and Dahyot, R.},
  LOCATION = {Derry-Londonderry, Northern Ireland},
  URL = {http://www.tara.tcd.ie/bitstream/handle/2262/71411/IMVIP2014_Proceedings.pdf},
  BOOKTITLE = {Irish Machine Vision and Image Processing Conference (IMVIP 2014)},
  DATE = {2014},
  NOTE = {URI http://hdl.handle.net/2262/71411},
  PAGES = {187--188},
  TITLE = {An Architecture for Social Media Summarisation},
}

@INPROCEEDINGS{Zdziarski:SIU:2014,
  ABSTRACT = {Visual saliency has been studied extensively in the past decades through perceptual studies using eye tracking technologies and 2D displays. Visual saliency algorithms have been successfully developed to mimick the human ability to quickly spot informative local areas in images. This paper proposes to investigate the extension of visual saliency algorithms to media displayed in 3D. We show first that the Graph-Based Visual Saliency (GBVS) algorithm outperforms all the other common 2D algorithms as well as their 3D extensions. This paper then extends GBVS to 3D and shows that these new 3D GBVS based algorithms outperform other past algorithms.},
  AUTHOR = {Zdziarski, Z. and Dahyot, R.},
  URL = {https://mural.maynoothuniversity.ie/15265/1/RD_extension.pdf},
  BOOKTITLE = {Signal Processing and Communications Applications Conference (SIU), 2014 22nd},
  DATE = {2014-04},
  DOI = {10.1109/SIU.2014.6830723},
  KEYWORDS = {computer vision;object tracking;2D displays;3D media;GBVS algorithm;GBVS extension;eye tracking technologies;graph-based visual saliency algorithm;Algorithm design and analysis;Computational modeling;Conferences;Signal processing algorithms;Stereo image processing;Three-dimensional displays;Visualization;3D media;Visual saliency},
  PAGES = {2296--2300},
  TITLE = {Extension of GBVS to 3D media},
}

@INPROCEEDINGS{ICPR2014Dahyot,
  ABSTRACT = {We compare the objective functions used by GR2T and the L2E estimator that have both been proposed for robust parameter estimation. We show their similarity when estimating location parameters. Of particular interest is their ability for dealing with the scale parameter that is often unknown and acts as a nuisance parameter. Both techniques are tested experimentally for regression (e.g. to find patterns such as line and circle in noisy datasets) and for registration between datasets.},
  AUTHOR = {Dahyot, R.},
  URL = {https://mural.maynoothuniversity.ie/15266/1/RD_GR2t.pdf},
  BOOKTITLE = {2014 22nd International Conference on Pattern Recognition},
  DATE = {2014-08},
  DOI = {10.1109/ICPR.2014.662},
  ISSN = {1051-4651},
  KEYWORDS = {Radon transforms;regression analysis;GR2T;L2E;generalized relaxed Radon transform;nuisance scale;regression analysis;robust parameter estimation;scale parameter;Equations;Estimation;Mathematical model;Noise;Pattern recognition;Robustness;Transforms},
  PAGES = {3857--3861},
  TITLE = {GR2T vs L2E with Nuisance Scale},
}

@INPROCEEDINGS{GroganIMVIP2014,
  ABSTRACT = {This paper proposes an algorithm for inferring a 3D mesh using the robust cost function proposed by Ruttle et al. Our contribution is in proposing a new algorithm for inference that is very suitable for parallel architecture. The cost function also provides a goodness of fit for each element of the mesh which is correlated to the distance to the ground truth, hence providing informative feedback to users.},
  AUTHOR = {Grogan, M. and Dahyot, R.},
  LOCATION = {Derry-Londonderry, Northern Ireland},
  URL = {http://www.tara.tcd.ie/bitstream/handle/2262/71411/IMVIP2014_Proceedings.pdf},
  BOOKTITLE = {Irish Machine Vision and Image Processing Conference},
  DATE = {2014},
  KEYWORDS = {3D reconstruction,Depth images,Generalised Relaxed Radon Transform},
  NOTE = {URI: http://hdl.handle.net/2262/71411},
  PAGES = {15--20},
  TITLE = {Mesh from Depth Images Using GR2T},
}

@INPROCEEDINGS{graisearchCIMU2014,
  ABSTRACT = {The amount of media that is being uploaded to social sites (such as Twitter, Facebook and Instagram) is providing a wealth of visual data (images and videos) augmented with additional information such as keywords, timestamps and GPS coordinates. Tapastreet provides access in real-time to this visual content by harvesting social networks for visual media associated with particular locations, time and hashtags [1]. Browsing efficiently through harvested videos requires smart processing to give users a quick overview of their content in particular when using mobile platforms with limited bandwidth. This paper aims at presenting an architecture for testing several strategies for processing summaries of videos collected on social networks to tackle this issue.},
  AUTHOR = {Zdziarski, Z. and Bourgès, C. and Mitchell, J. and Houdyer, P. and Johnson, D. and Dahyot, R.},
  LOCATION = {Paris, France},
  URL = {https://mural.maynoothuniversity.ie/15268/1/RD_on%20summarising.pdf},
  BOOKTITLE = {2014 International Workshop on Computational Intelligence for Multimedia Understanding (IWCIM)},
  DATE = {2014},
  DOI = {10.1109/IWCIM.2014.7008797},
  KEYWORDS = {mobile computing;social networking (online);video retrieval;video signal processing;Tapastreet;mobile platforms;smart mobile browsing;smart processing;social networks;social sites;social video summarisation;visual content;visual data;visual media;Media;Pipelines;Social network services;Streaming media;Transform coding;Videos;Visualization;Blur Detection;MPEG Codec;Social Media;Video Summarisation;Web Harvesting},
  TITLE = {On summarising the 'here and now' of social videos for smart mobile browsing},
}

@ARTICLE{RuttlePRL2014,
  ABSTRACT = {This paper proposes to infer accurately a 3D shape of an object captured by a depth camera from multiple view points. The Generalised Relaxed Radon Transform (GR2T) [1] is used here to merge all depth images in a robust kernel density estimate that models the surface of an object in the 3D space. The kernel is tailored to capture the uncertainty associated with each pixel in the depth images. The resulting cost function is suitable for stochastic exploration with gradient ascent algorithms when the noise of the observations is modelled with a differentiable distribution. When merging several depth images captured from several view points, extrinsic camera parameters need to be known accurately, and we extend GR2T to also estimate these nuisance parameters. We illustrate qualitatively the performance of our modelling and we assess quantitatively the accuracy of our 3D shape reconstructions computed from depth images captured with a Kinect camera.},
  AUTHOR = {Ruttle, Jonathan and Arellano, Claudia and Dahyot, Rozenn},
  URL = {http://www.tara.tcd.ie/bitstream/handle/2262/68177/Ruttle%2D%2DRobust%20shape%20from%20de.pdf},
  DATE = {2014},
  DOI = {10.1016/j.patrec.2014.01.016},
  ISSN = {0167-8655},
  JOURNALTITLE = {Pattern Recognition Letters},
  KEYWORDS = {Shape from depth,Generalised Relaxed Radon Transform (GRT),Noise modelling},
  NOTE = {URI: http://hdl.handle.net/2262/68177},
  PAGES = {43--54},
  TITLE = {Robust shape from depth images with GR2T},
  VOLUME = {50},
}

@ARTICLE{DSP2013Dahyot,
  ABSTRACT = {This paper introduces a smooth posterior density function for inferring shapes from silhouettes. Both the likelihood and the prior are modelled using kernel density functions and optimisation is performed using gradient ascent algorithms. Adding a prior allows for the recovery of concave areas of the shape that are usually lost when estimating the visual hull. This framework is also extended to use colour information when it is available in addition to the silhouettes. In these cases, the modelling not only allows for the shape to be recovered but also its colour information. Our new algorithms are assessed by reconstructing 2D shapes from 1D silhouettes and 3D faces from 2D silhouettes. Experimental results show that using the prior can assist in reconstructing concave areas and also illustrate the benefits of using colour information even when only small numbers of silhouettes are available.},
  AUTHOR = {Kim, Donghoon and Ruttle, Jonathan and Dahyot, Rozenn},
  URL = {https://mural.maynoothuniversity.ie/15118/1/RD_bayesian.pdf},
  DATE = {2013},
  DOI = {10.1016/j.dsp.2013.06.007},
  ISSN = {1051-2004},
  JOURNALTITLE = {Digital Signal Processing},
  KEYWORDS = {3D reconstruction from multiple view images,Shape-from-silhouettes,Kernel density estimates,K-nearest neighbours,Principal component analysis},
  NUMBER = {6},
  PAGES = {1844--1855},
  TITLE = {Bayesian 3D shape from silhouettes},
  VOLUME = {23},
}

@ARTICLE{PR2012Dahyot,
  ABSTRACT = {This paper introduces the generalised relaxed Radon transform (GR2T) as an extension to the generalised radon transform (GRT) . This new modelling allows us to define a new framework for robust inference. The resulting objective functions are probability density functions that can be chosen differentiable and that can be optimised using gradient methods. One of this cost function is already widely used in the forms of the Hough transform and generalised projection based M-estimator, and it is interpreted as a conditional density function on the latent variables of interest. In addition the joint density function of the latent variables is also proposed as a cost function and it has the advantage of including a prior about the latent variable. Several applications, including lines detection in images and volume reconstruction from silhouettes captured from multiple views, are presented to underline the versatility of this framework.},
  AUTHOR = {Dahyot, Rozenn and Ruttle, Jonathan},
  URL = {https://mural.maynoothuniversity.ie/15119/1/RD_generalised.pdf},
  DATE = {2013},
  DOI = {10.1016/j.patcog.2012.09.026},
  ISSN = {0031-3203},
  JOURNALTITLE = {Pattern Recognition},
  KEYWORDS = {Generalised Radon transform,Hough transform,Robust inference,M-estimator,Generalised projection based M-estimator},
  NUMBER = {3},
  PAGES = {788--794},
  TITLE = {Generalised relaxed Radon transform (GR2T) for robust inference},
  VOLUME = {46},
}

@INPROCEEDINGS{Zdziarski:2013:ACM:SAP,
  AUTHOR = {Zdziarski, Z. and Dahyot, R.},
  LOCATION = {Dublin, Ireland},
  PUBLISHER = {ACM},
  URL = {http://doi.acm.org/10.1145/2492494.2501889},
  BOOKTITLE = {Proceedings of the ACM Symposium on Applied Perception},
  DATE = {2013},
  DOI = {http://doi.org/10.1145/2492494.2501889},
  ISBN = {978-1-4503-2262-1},
  PAGES = {132--132},
  SERIES = {SAP '13},
  TITLE = {On Creating a 2D \& 3D Visual Saliency Dataset},
}

@INPROCEEDINGS{CVMP2013Arellano,
  ABSTRACT = {We propose to fit automatically a 3D morphable face model to a point cloud captured with a RGB-D sensor. Both data sets, the shape model and the target point cloud are modelled as two probability density functions (pdfs). Rigid registration (rotation and translation) and reconstruction on the model is performed by minimising the Euclidean distance between these two pdfs augmented with a multivariate Gaussian prior. Our resulting process is robust and it does not require point to point correspondence. Experimental results on synthetic and real data illustrates the performance of this novel approach.},
  AUTHOR = {Arellano, C. and Dahyot, R.},
  LOCATION = {London, United Kingdom},
  PUBLISHER = {ACM},
  URL = {https://roznn.github.io/PDF/RzDCVMP2013.pdf},
  BOOKTITLE = {Proceedings of the 10th European Conference on Visual Media Production},
  DATE = {2013},
  DOI = {10.1145/2534008.2534013},
  ISBN = {978-1-4503-2589-9},
  KEYWORDS = {3D face reconstruction,L2E,RGB-D sensor,computer vision,divergence,morphable models,registration,shape fitting},
  NOTE = {http://doi.acm.org/10.1145/2534008.2534013},
  PAGES = {9:1--9:10},
  SERIES = {CVMP '13},
  TITLE = {Robust Bayesian Fitting of 3D Morphable Model},
}

@INPROCEEDINGS{KimMuscle2011,
  ABSTRACT = {This paper extends the likelihood kernel density estimate of the visual hull proposed by Kim et al [1] by introducing a prior. Inference of the shape is performed using a meanshift algorithm over a posterior kernel density function that is refined iteratively using both a multiresolution framework (to avoid local maxima) and using KNN for selecting the best reconstruction basis at each iteration. This approach allows us to recover concave areas of the shape that are usually lost when estimating the visual hull.},
  AUTHOR = {Kim, Donghoon and Dahyot, Rozenn},
  EDITOR = {Salerno, Emanuele and Çetin, A. Enis and Salvetti, Ovidio},
  LOCATION = {Berlin, Heidelberg},
  PUBLISHER = {Springer Berlin Heidelberg},
  BOOKTITLE = {Computational Intelligence for Multimedia Understanding},
  DATE = {2012},
  DOI = {10.1007/978-3-642-32436-9_7},
  ISBN = {978-3-642-32436-9},
  PAGES = {78--89},
  TITLE = {Bayesian Shape from Silhouettes},
}

@INPROCEEDINGS{6334154,
  ABSTRACT = {3D reconstruction from multiple view images requires that camera parameters are very accurately known and standard camera calibration techniques [1] often fail to provide the required level of accuracy for the extrinsic camera parameters. Using the Kinect depth camera, we propose to estimate camera parameters by minimising the cross correlation between density functions modelled for each recorded depth images. We illustrate experimentally how this improves the modelling for estimating 3D shape from Depths.},
  AUTHOR = {Ruttle, J. and Arellano, C. and Dahyot, R.},
  URL = {https://www.eurasip.org/Proceedings/Eusipco/Eusipco2012/Conference/papers/1569583097.pdf},
  BOOKTITLE = {2012 Proceedings of the 20th European Signal Processing Conference (EUSIPCO)},
  DATE = {2012-08},
  EPRINT = {https://ieeexplore.ieee.org/document/6334154},
  ISSN = {2219-5491},
  KEYWORDS = {calibration;cameras;correlation methods;image reconstruction;parameter estimation;extrinsic camera parameter estimation;shape-from-depths;3D reconstruction;multiple view images;standard camera calibration techniques;kinect depth camera;recorded depth images;Cameras;Calibration;Cost function;Shape;Probability density function;Computational modeling;Correlation;Shape-from-Silhouettes (SfS);Shape-from-Depths (SfD);Multiview geometry},
  PAGES = {1985--1989},
  TITLE = {Extrinsic camera parameters estimation for shape-from-depths},
}

@INPROCEEDINGS{ISSC2012Ziggy,
  ABSTRACT = {Saliency algorithms in content-based image retrieval are employed to retrieve the most important regions of an image with the idea that these regions hold the essence of representative information. Such regions are then typically analysed and described for future retrieval/classification tasks rather than the entire image itself - thus minimising computational resources required. We show that we can select a small number of features for indexing using a visual saliency measure without reducing the performance of classifiers trained to find objects.},
  AUTHOR = {Zdziarski, Z. and Dahyot, R.},
  LOCATION = {Maynooth, Ireland},
  URL = {https://mural.maynoothuniversity.ie/15270/1/RD_feature.pdf},
  BOOKTITLE = {23nd IET Irish Signals and Systems Conference},
  DATE = {2012-06},
  DOI = {10.1049/ic.2012.0194},
  NOTE = {https://ieeexplore.ieee.org/document/6621173/},
  TITLE = {Feature Selection Using Visual Saliency for Content-Based Image Retrieval},
}

@INPROCEEDINGS{6334159,
  ABSTRACT = {We present a Mean shift (MS) algorithm for solving the rigid point set transformation estimation [1]. Our registration algorithm minimises exactly the Euclidean distance between Gaussian Mixture Models (GMMs). We show experimentally that our algorithm is more robust than previous implementations [1], thanks to both using an annealing framework (to avoid local extrema) and using variable bandwidths in our density estimates. Our approach is applied to 3D real data sets captured with a Lidar scanner and Kinect sensor.},
  AUTHOR = {{Arellano}, C. and {Dahyot}, R.},
  URL = {https://www.eurasip.org/Proceedings/Eusipco/Eusipco2012/Conference/papers/1569583125.pdf},
  BOOKTITLE = {2012 Proceedings of the 20th European Signal Processing Conference (EUSIPCO)},
  DATE = {2012-08},
  EPRINT = {https://ieeexplore.ieee.org/document/6334159},
  ISSN = {2219-5491},
  KEYWORDS = {Gaussian processes;image registration;image sensors;optical radar;optical scanners;mean shift algorithm;robust rigid registration algorithm;Gaussian mixture models;MS algorithm;rigid point set transformation estimation;Euclidean distance;GMM;annealing framework;density estimation;3D real data sets;lidar scanner;Kinect sensor;Bandwidth;Density functional theory;Robustness;Kernel;Estimation;Annealing;Cost function;Mean Shift;Registration;Gaussian Mixture Models;Rigid Transformation},
  PAGES = {1154--1158},
  TITLE = {Mean shift algorithm for robust rigid registration between Gaussian Mixture Models},
}

@ARTICLE{IJCV2012Cem,
  ABSTRACT = {We present a novel and effective skeletonization algorithm for binary and gray-scale images, based on the anisotropic heat diffusion analogy. We diffuse the image in the direction normal to the feature boundaries and also allow tangential diffusion (curvature decreasing diffusion) to contribute slightly. The proposed anisotropic diffusion provides a high quality medial function in the image: it removes noise and preserves prominent curvatures of the shape along the level-sets (skeleton features). The skeleton strength map, which provides the likelihood of a point to be part of the skeleton, is defined by the mean curvature measure. Finally, thin and binary skeleton is obtained by non-maxima suppression and hysteresis thresholding of the skeleton strength map. Our method outperforms the most related and the popular methods in skeleton extraction especially in noisy conditions. Results show that the proposed approach is better at handling noise in images and preserving the skeleton features at the centerline of the shape.},
  AUTHOR = {Direkoglu, Cem and Dahyot, Rozenn and Manzke, Michael},
  URL = {https://mural.maynoothuniversity.ie/15120/1/RD_on%20using.pdf},
  DATE = {2012-11-01},
  DOI = {10.1007/s11263-012-0540-9},
  ISSN = {1573-1405},
  JOURNALTITLE = {International Journal of Computer Vision},
  NUMBER = {2},
  PAGES = {170--189},
  TITLE = {On Using Anisotropic Diffusion for Skeleton Extraction},
  VOLUME = {100},
}

@INPROCEEDINGS{ISSC2012Claudia,
  ABSTRACT = {We present a Mean Shift algorithm for fitting shape models. This algorithm maximises a posterior density function where the likelihood is defined as the Euclidean distance between two Gaussian mixture density functions, one modelling the observations while the other corresponds to the shape model. We explore the role of the covariance matrix in the Gaussian kernel for encoding the shape of the model in the density function. Results show that using non-isotropic covariance matrices improve the efficiency of the algorithm and allow to reduce the number of kernels to use in the mixture without compromising the robustness of the algorithm.},
  AUTHOR = {Arellano, C. and Dahyot, R.},
  LOCATION = {Maynooth, Ireland},
  URL = {https://mural.maynoothuniversity.ie/15273/1/RD_shape.pdf},
  BOOKTITLE = {IET Irish Signals and Systems Conference (ISSC 2012)},
  DATE = {2012-06},
  DOI = {10.1049/ic.2012.0196},
  KEYWORDS = {Gaussian processes;covariance matrices;solid modelling;Euclidean distance;Gaussian kernel;Gaussian mixture density functions;mean shift algorithm;nonisotropic GMM;nonisotropic covariance matrices;posterior density function;shape model fitting;Fitting Algorithm;Gaussian Mixture Models;Mean Shift;Morphable Models},
  PAGES = {1--6},
  TITLE = {Shape Model Fitting Using non-Isotropic GMM},
}

@INPROCEEDINGS{Eusipco2012Arellano1,
  ABSTRACT = {In this paper, we present a Mean Shift algorithm that does not require point correspondence to fit shape models. The observed data and the shape model are represented as mixtures of Gaussians. Using a Bayesian framework, we propose to model the likelihood using the Euclidean distance between the two Gaussian mixture density functions while the latent variables are modelled with a Gaussian prior. We show the performance of our MS algorithm for fitting a 2D hand model and a 3D Morphable Model of faces to point clouds.},
  AUTHOR = {Arellano, C. and Dahyot, R.},
  URL = {https://www.eurasip.org/Proceedings/Eusipco/Eusipco2012/Conference/papers/1569582293.pdf},
  BOOKTITLE = {2012 Proceedings of the 20th European Signal Processing Conference (EUSIPCO)},
  DATE = {2012-08},
  EPRINT = {https://ieeexplore.ieee.org/document/6333999/},
  ISSN = {2219-5491},
  KEYWORDS = {Gaussian processes;shape recognition;2D hand model;3D morphable model;Bayesian framework;Euclidean distance;Gaussian mixture density functions;Gaussian prior;Gaussians mixtures;MS algorithm;mean shift algorithm;shape model fitting algorithm;Computational modeling;Data models;Euclidean distance;Robustness;Shape;Signal processing algorithms;Solid modeling;Gaussian Mixture Models;Mean Shift;Morphable Models;Shape Fitting},
  PAGES = {934--938},
  TITLE = {Shape model fitting algorithm without point correspondence},
}

@ARTICLE{Risser2010,
  ABSTRACT = {Example-based texture synthesis algorithms generate novel texture images from example data. A popular hierarchical pixel-based approach uses spatial jitter to introduce diversity, at the risk of breaking coarse structure beyond repair. We propose a multiscale descriptor that enables appearance-space jitter, which retains structure. This idea enables repurposing of existing texture synthesis implementations for a qualitatively different problem statement and class of inputs: generating hybrids of structured images.},
  AUTHOR = {Risser, Eric and Han, Charles and Dahyot, Rozenn and Grinspun, Eitan},
  LOCATION = {New York, NY, USA},
  PUBLISHER = {ACM},
  URL = {http://www.cs.columbia.edu/cg/hybrids/hybrids.pdf},
  DATE = {2010-07},
  DOI = {10.1145/1778765.1778822},
  EPRINT = {http://doi.acm.org/10.1145/1778765.1778822},
  ISSN = {0730-0301},
  JOURNALTITLE = {ACM Trans. Graph.},
  NUMBER = {4},
  PAGES = {85:1--85:6},
  TITLE = {Synthesizing Structured Image Hybrids},
  VOLUME = {29},
}

@INPROCEEDINGS{Kim2010Icassp,
  ABSTRACT = {In this article, a novel method to accurately estimate 3D surface of objects of interest is proposed. Each ray projected from 2D image plane to 3D space is modelled with the Gaussian kernel function. Then a mean shift algorithm with an annealing scheme is used to find maximums of the probability density function and recovers the 3D surface. Experimental results show that our method is more accurate to estimate 3D surface than the Radon transform-based approach.},
  AUTHOR = {Kim, D. and Ruttle, J. and Dahyot, R.},
  URL = {https://mural.maynoothuniversity.ie/15281/1/RD_3D%20shape.pdf},
  BOOKTITLE = {IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP 2010)},
  DATE = {2010-03},
  DOI = {10.1109/ICASSP.2010.5495474},
  ISSN = {1520-6149},
  PAGES = {1430--1433},
  TITLE = {3D shape estimation from silhouettes using Mean-shift},
}

@INBOOK{LCPC2010,
  AUTHOR = {Charbonnier, P. and Dahyot, R. and Vik, T. and Heitz, F.},
  PUBLISHER = {Etudes et Recherches des laboratoires des Ponts et Chaussées, CR53 ( ISBN 978-2-7208-2578-1)},
  CHAPTER = {Détection et Reconnaissance de la signalisation verticale par analyse d'images},
  DATE = {2010-07},
  TITLE = {Detection et reconnaissance de la signalisation verticale par analyse d’images (Ed: P. Foucher)},
}

@INPROCEEDINGS{Direkoglu2010,
  ABSTRACT = {We introduce a novel skeleton extraction algorithm in binary and gray-scale images, based on the anisotropic heat diffusion analogy. We propose to diffuse image in the dominance of direction normal to the feature boundaries and also allow tangential diffusion to contribute slightly. The proposed anisotropic diffusion provides a high quality medial function in the image, since it removes noise and preserves prominent curvatures of the shape along the level-sets (skeleton locations). Then the skeleton strength map, which provides the likelihood to be a skeleton point, is obtained by computing the mean curvature of level-sets. The overall process is completed by non-maxima suppression and hysteresis thresholding to obtain thin and binary skeleton. Results indicate that this approach has advantages in handling noise in the image and in obtaining smooth shape skeleton because of the directional averaging inherent of our new anisotropic heat flow.},
  AUTHOR = {Direkoglu, Cem and Dahyot, Rozenn and Manzke, Michael},
  PUBLISHER = {BMVA Press},
  URL = {http://www.bmva.org/bmvc/2010/conference/paper61/paper61.pdf},
  BOOKTITLE = {Proceedings of the British Machine Vision Conference},
  DATE = {2010},
  DOI = {10.5244/C.24.61},
  ISBN = {1-901725-40-5},
  PAGES = {61.1--61.11},
  TITLE = {Skeleton Extraction via Anisotropic Heat Flow},
}

@INPROCEEDINGS{Ruttle2010CVMP,
  ABSTRACT = {We present a statistical framework to merge the information from silhouettes segmented in multiple view images to infer the 3D shape of an object. The approach is generalising the robust but discrete modelling of the visual hull by using the concept of averaged likelihoods. One resulting advantage of our framework is that the objective function is continuous and therefore an iterative gradient ascent algorithm can be defined to efficiently search the space. Moreover this results in a method which is less memory demanding and one that is very suitable to a parallel processing architecture. Experimental results shows that this approach is efficient for getting a robust initial guess to the 3D shape of an object in view.},
  AUTHOR = {Ruttle, J. and Manzke, M. and Dahyot, R.},
  LOCATION = {London UK},
  URL = {https://mural.maynoothuniversity.ie/15282/1/RD_smooth.pdf},
  BOOKTITLE = {proceedings of The 7th European Conference for Visual Media Production, CVMP 2010},
  DATE = {2010},
  DOI = {10.1109/CVMP.2010.17},
  PAGES = {74--81},
  TITLE = {Smooth Kernel Density Estimate for Multiple View Reconstruction},
}

@INPROCEEDINGS{Arellano2010,
  AUTHOR = {Arellano, C. and Dahyot, R.},
  LOCATION = {Limerick Ireland},
  URL = {http://citeseerx.ist.psu.edu/viewdoc/download?doi = 10.1.1.394.4985&rep = rep1&type = pdf},
  BOOKTITLE = {International Machine Vision and Image Processing Conference (IMVIP 2010)},
  DATE = {2010-09},
  TITLE = {Stereo Images for 3D Face Applications: A Literature Review},
}

@INPROCEEDINGS{Kim09IMVIP,
  ABSTRACT = {Given information from many cameras, one can hope to get a complete 3D representation of an object. Pintavirooj and Sangworasil exploit this idea and present a system that records sequentially images from multiple view points to reconstruct a 3D shape of a static object of interest [1]. For instance, using a 60 angle of view on the image, they manage to get its accurate 3D reconstruction [1]. Unfortunately, when considering application such as video surveillance, it is not reasonable to expect that 60 cameras will give simultaneous images of a person of interest. However, we can expect that the person will move over time and show sequentially different poses of her/his head to at least one or a few cameras. This article proposes a technique for recovering an accurate 3D shape by combining views recorded at different times.},
  AUTHOR = {Kim, D. and Dahyot, R.},
  LOCATION = {Dublin, Ireland},
  URL = {https://ieeexplore.ieee.org/document/5319298/},
  BOOKTITLE = {International Machine Vision and Image Processing conference (IMVIP 2009)},
  DATE = {2009-09},
  DOI = {10.1109/IMVIP.2009.35},
  PAGES = {156--161},
  TITLE = {3D Head Reconstruction using Multi-camera Stream},
}

@INPROCEEDINGS{Ruttle09Imvip,
  ABSTRACT = {Scene flow is the motion of the surface points in the 3D world. For a camera, it is seen as a 2D optical flow in the image plane. Knowing the scene flow can be very useful as it gives an idea of the surface geometry of the objects in the scene and how those objects are moving. Four methods for calculating the scene flow given multiple optical flows have been explored and detailed in this paper along with the basic mathematics surrounding multi-view geometry. It was found that given multiple optical flows it is possible to estimate the scene flow to different levels of detail depending on the level of prior information present.},
  AUTHOR = {Ruttle, J. and Manzke, M. and Dahyot, R.},
  LOCATION = {Dublin, Ireland},
  URL = {https://mural.maynoothuniversity.ie/15285/1/RD_estimating.pdf},
  BOOKTITLE = {International Machine Vision and Image Processing Conference (IMVIP 2009)},
  DATE = {2009-09},
  DOI = {10.1109/IMVIP.2009.8},
  NOTE = {URI: http://hdl.handle.net/2262/30634},
  PAGES = {6--11},
  TITLE = {Estimating 3D Scene Flow from Multiple 2D Optical Flows},
}

@REPORT{Dahyot09TR,
  AUTHOR = {Dahyot, R.},
  INSTITUTION = {School of Computer Science and Statistics, Trinity College Dublin},
  URL = {https://www.scss.tcd.ie/disciplines/statistics/tech-reports/09-01.pdf},
  DATE = {2009-04},
  NUMBER = {01/09},
  TITLE = {Mean-shift for Statistical Hough Transform},
  TYPE = {techreport},
}

@INPROCEEDINGS{Zdziarski09Imvip,
  ABSTRACT = {We present an algorithm for slideshow detection in video databases such as YouTube or Blip.TV. Our solution is based around feature tracking to extract movement between sequentially captured frames. This movement is then analysed through the use of the Hough transform and compared against behaviour commonly exhibited by slideshows: still and panning static images. We show experimentally the effectiveness of this novel idea and approach.},
  AUTHOR = {Zdziarski, Z. and Dahyot, R.},
  LOCATION = {Dublin, Ireland},
  URL = {https://mural.maynoothuniversity.ie/15284/1/RD_robust.pdf},
  BOOKTITLE = {International Machine Vision and Image Processing Conference (IMVIP 2009)},
  DATE = {2009-09},
  DOI = {10.1109/IMVIP.2009.23},
  PAGES = {89--93},
  TITLE = {Robust Panning Analysis for Slideshow Detection in Video Databases},
}

@ARTICLE{Dahyot08pami,
  ABSTRACT = {The standard Hough transform is a popular method in image processing and is traditionally estimated using histograms. Densities modeled with histograms in high dimensional space and/or with few observations, can be very sparse and highly demanding in memory. In this paper, we propose first to extend the formulation to continuous kernel estimates. Second, when dependencies in between variables are well taken into account, the estimated density is also robust to noise and insensitive to the choice of the origin of the spatial coordinates. Finally, our new statistical framework is unsupervised (all needed parameters are automatically estimated) and flexible (priors can easily be attached to the observations). We show experimentally that our new modeling encodes better the alignment content of images.},
  AUTHOR = {Dahyot, R.},
  URL = {https://mural.maynoothuniversity.ie/15126/1/RD_stat.pdf},
  DATE = {2009-08},
  DOI = {10.1109/TPAMI.2008.288},
  EPRINT = {http://www.tara.tcd.ie/handle/2262/31106},
  ISSN = {0162-8828},
  JOURNALTITLE = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  KEYWORDS = {Hough transforms;object detection;statistical analysis;continuous kernel estimate;image processing;line detection;spatial domain coordinate;statistical Hough transform;Hough transform;Image Processing and Computer Vision;Radon transform;Transform methods;kernel probability density function;line detection.;uncertainty},
  NOTE = {URI: http://hdl.handle.net/2262/31106 - Github: https://github.com/Roznn/Statistical-Hough-Transform},
  NUMBER = {8},
  PAGES = {1502--1509},
  TITLE = {Statistical Hough Transform},
}

@INPROCEEDINGS{Ruttle09Siggraph,
  ABSTRACT = {This work addresses the challenge of synchronizing multiple sources of visible and audible information from a variety of devices, while capturing human motion in realtime. Video and audio data will be used to augment and enrich a motion capture database that will be released to the research community. While other such augmented motion capture databases exist [Black and Sigal 2006], the goal of this work is to build on these previous works. Critical areas of improvement are in the synchronization between cameras and synchronization between devices. Adding an array of audio recording devices to the setup will greatly expand the research potential of the database, and the positioning of the cameras will be varied to give greater flexibility. The augmented database will facilitate the testing and validation of human pose estimation and motion tracking techniques, among other applications. This sketch briefly describes some of the interesting challenges faced in setting up the pipeline for capturing the synchronized data and the novel approaches proposed to solve them.},
  AUTHOR = {Ruttle, J. and Manzke, M. and Prazak, M. and Dahyot, R.},
  LOCATION = {Yokohama, Japan},
  PUBLISHER = {ACM},
  BOOKTITLE = {SIGGRAPH ASIA '09: ACM SIGGRAPH ASIA 2009 Posters},
  DATE = {2009},
  DOI = {10.1145/1666778.1666828},
  PAGES = {1--1},
  TITLE = {Synchronized real-time multi-sensor motion capture system},
}

@INPROCEEDINGS{KearneyAES08,
  ABSTRACT = {We present an integrated set of audio-visual tracking and synthesis tools to aid matching of the audio to the video position in both horizontal and periphonic sound reinforcement systems. Compensation for screen size and loudspeaker layout for high definition formats is incorporated and the spatial localisation of the source is rendered using advanced spatialisation techniques. A subjective comparison of several original and enhanced film sequences using the Vector Base Amplitude Panning (VBAP) method is presented. The results show that the encoding of non-contradictory audio-visual spatial information, for presentation on different loudspeaker layouts significantly improves the naturalness of the listening/viewing experience.},
  AUTHOR = {Kearney, G. and Dahyot, R. and Boland, F.},
  URL = {http://www.aes.org/e-lib/browse.cfm?elib = 14495},
  BOOKTITLE = {Audio Engineering Society 134th Convention},
  DATE = {2008-05},
  TITLE = {Audio-Visual Processing Tools for Auditory Scene Synthesis},
}

@INPROCEEDINGS{Dahyoticpr08,
  ABSTRACT = {We have introduced the statistical Hough transform that extends the standard Hough transform by using a kernel mixture as a robust alternative to the 2 dimensional accumulator histogram. This work develops further this framework by proposing a Bayesian classification scheme to associate the spatial coordinates (x, y) to one particular class defined in the Hough space. In a first step, we segment the Hough space into meaningful classes. Then using the inverse Radon transform, we backproject the different classes into the image space. We illustrate our approach on a synthetic image and on real images.},
  AUTHOR = {Dahyot, R.},
  LOCATION = {Tampa, Florida},
  BOOKTITLE = {2008 19th International Conference on Pattern Recognition},
  DATE = {2008-12},
  DOI = {10.1109/ICPR.2008.4761109},
  ISSN = {1051-4651},
  KEYWORDS = {Bayes methods;Hough transforms;Radon transforms;image classification;image segmentation;statistical analysis;2D accumulator histogram;Bayesian classification scheme;image space;inverse Radon transform;kernel mixture;statistical Hough transform;Bandwidth;Bayesian methods;Computer science;Discrete transforms;Educational institutions;Histograms;Image segmentation;Kernel;Robustness;Statistics},
  PAGES = {1--4},
  TITLE = {Bayesian Classification for the Statistical Hough Transform},
}

@INPROCEEDINGS{Donghoon08imvip,
  ABSTRACT = {We present a feature-based method to classify salient points as belonging to objects in the face or background classes. We use SURF local descriptors (speeded up robust features) to generate feature vectors and use SVMs (support vector machines) as classifiers. Our system consists of a two-layer hierarchy of SVMs classifiers. On the first layer, a single classifier checks whether feature vectors are from face images or not. On the second layer, component labeling is operated using each component classifier of eye, mouth, and nose. This approach has the advantage about operating time because windows scanning procedure is not needed. Finally, this system performs the procedure to apply geometrical constraints to labeled descriptors. We show experimentally the efficiency of our approach.},
  AUTHOR = {Kim, D. and Dahyot, R.},
  URL = {https://mural.maynoothuniversity.ie/15287/1/RD_face.pdf},
  BOOKTITLE = {International Machine Vision and Image Processing conference (IMVIP 2008)},
  DATE = {2008},
  DOI = {10.1109/IMVIP.2008.15},
  TITLE = {Face components detection using SURF descriptor and SVMs},
}

@ARTICLE{Dahyot08,
  ABSTRACT = {Colonoscopy is currently one of the best methods to detect colorectal cancer. Nowadays, one of the widely used colonoscopes has a monochrome chipset recording successively at 60Hz and components merged into one color video stream. Misalignments of the channels occur each time the camera moves, and this artefact impedes both online visual inspection by doctors and offline computer analysis of the image data. We propose to restore this artefact by first equalizing the color channels and then performing a robust camera motion estimation and compensation.},
  AUTHOR = {Dahyot, Rozenn and Vilariño, Fernando and Lacey, Gerard},
  URL = {https://mural.maynoothuniversity.ie/15123/1/RD_improving.pdf},
  DATE = {2008-01-22},
  DOI = {10.1155/2008/139429},
  EPRINT = {https://jivp-eurasipjournals.springeropen.com/articles/10.1155/2008/139429},
  ISSN = {1687-5281},
  JOURNALTITLE = {EURASIP Journal on Image and Video Processing},
  NUMBER = {1},
  PAGES = {139429},
  TITLE = {Improving the Quality of Color Colonoscopy Videos},
}

@INBOOK{Wilson2008,
  ABSTRACT = {Bayesian methods are a class of statistical methods that have some appealing properties for solving problems in machine learning, particularly when the process being modelled has uncertain or random aspects. In this chapter we look at the mathematical and philosophical basis for Bayesian methods and how they relate to machine learning problems in multimedia. We also discuss the notion of decision theory, for making decisions under uncertainty, that is closely related to Bayesian methods. The numerical methods needed to implement Bayesian solutions are also discussed. Two specific applications of the Bayesian approach that are often used in machine learning -- naïve Bayes and Bayesian networks -- are then described in more detail.},
  AUTHOR = {Wilson, Simon P. and Dahyot, Rozenn and Cunningham, Pádraig},
  LOCATION = {Berlin, Heidelberg},
  PUBLISHER = {Springer Berlin Heidelberg (Eds: Cord, Matthieu and Cunningham, Pádraig)},
  URL = {https://doi.org/10.1007/978-3-540-75171-7_1},
  CHAPTER = {Introduction to Bayesian Methods and Decision Theory},
  DATE = {2008},
  DOI = {10.1007/978-3-540-75171-7{\_1}},
  ISBN = {978-3-540-75171-7},
  PAGES = {3--19},
  TITLE = {Machine Learning Techniques for Multimedia: Case Studies on Organization and Retrieval},
}

@INBOOK{DahyotChapter2008,
  AUTHOR = {Dahyot, Rozenn and Pitie, François and Lennon, Daire and Harte, Naomi and Kokaram, Anil},
  LOCATION = {Boston, MA},
  PUBLISHER = {Springer US (Eds: Maragos, Petros and Potamianos, Alexandros and Gros, Patrick)},
  URL = {https://doi.org/10.1007/978-0-387-76316-3_5},
  CHAPTER = {Action Recognition in Multimedia Streams},
  DATE = {2008},
  DOI = {10.1007/978-0-387-76316-3{\_}5},
  ISBN = {978-0-387-76316-3},
  TITLE = {Multimodal Processing and Interaction: Audio, Video, Text},
}

@INBOOK{PitieCRC2008,
  AUTHOR = {Pitie, F. and Kokaram, A. and Dahyot, R.},
  PUBLISHER = {CRC Press Image Processing Series, Rastislav Lukac (Ed.) ISBN: 9781420054521},
  URL = {https://github.com/frcs/colour-transfer/blob/master/publications/pitie08bookchapter.pdf},
  CHAPTER = {Enhancement of Digital Photographs Using Color Transfer Techniques},
  DATE = {2008-10},
  DOI = {10.1201/9781420054538.ch11},
  NOTE = {Github: https://github.com/frcs/colour-transfer},
  TITLE = {Single-Sensor Imaging: Methods and Applications for Digital Cameras},
}

@ARTICLE{Pitie_CVIU2007,
  ABSTRACT = {This article proposes an original method for grading the colours between different images or shots. The first stage of the method is to find a one-to-one colour mapping that transfers the palette of an example target picture to the original picture. This is performed using an original and parameter free algorithm that is able to transform any N-dimensional probability density function into another one. The proposed algorithm is iterative, non-linear and has a low computational cost. Applying the colour mapping on the original picture allows reproducing the same ‘feel’ as the target picture, but can also increase the graininess of the original picture, especially if the colour dynamic of the two pictures is very different. The second stage of the method is to reduce this grain artefact through an efficient post-processing algorithm that intends to preserve the gradient field of the original picture.},
  AUTHOR = {Pitie, François and Kokaram, Anil C. and Dahyot, Rozenn},
  URL = {https://mural.maynoothuniversity.ie/15125/1/RD_automated.pdf},
  DATE = {2007},
  DOI = {10.1016/j.cviu.2006.11.011},
  EPRINT = {http://www.sciencedirect.com/science/article/pii/S1077314206002189},
  ISSN = {1077-3142},
  JOURNALTITLE = {Computer Vision and Image Understanding},
  KEYWORDS = {Colour grading,Colour transfer,Re-colouring,Distribution transfer},
  NOTE = {Github: https://github.com/frcs/colour-transfer},
  NUMBER = {1},
  PAGES = {123--137},
  TITLE = {Automated colour grading using colour distribution transfer},
  VOLUME = {107},
}

@REPORT{Dahyot07,
  AUTHOR = {Dahyot, R.},
  INSTITUTION = {School of Computer Science and Statistics, Trinity College Dublin Ireland},
  URL = {https://www.cs.tcd.ie/publications/tech-reports/reports.07/TCD-CS-2007-37.pdf},
  DATE = {2007-07},
  NUMBER = {TCD-CS-2007-37},
  TITLE = {Statistical Hough Transform},
  TYPE = {techreport},
}

@REPORT{DahyotPIYRA07,
  AUTHOR = {Dahyot, R.},
  INSTITUTION = {School of Computer Science and Statistics, Trinity College Dublin Ireland},
  URL = {https://roznn.github.io/PDF/RzDPIYRA2007.pdf},
  DATE = {2007},
  NOTE = {First stage proposal TCD selection to PIYRA (not funded)},
  TITLE = {Optimal Mass Transport for Understanding and Synthesis of Visual Data},
  TYPE = {techreport},
}

@REPORT{DahyotLacey07,
  ABSTRACT = {We propose a method to restore colonoscopy videos that have low quality RGB images. The main problem concerns a time delay occurring in between the recordings of the R, G and B colour channels. As the camera is moving along in the colon, sometimes quickly, the resulting images show non properly matched R, G and B causing blurry effects that impede the medical doctors or computer-aided analysis methods. We proposed to restore this artefact by first equalizing the colour channels and then performing a robust camera motion estimation and compensation. Experimental results show significant improvements from the original videos.},
  AUTHOR = {Dahyot, R. and Lacey, G.},
  INSTITUTION = {School of Computer Science and Statistics, Trinity College Dublin Ireland},
  URL = {http://www.tara.tcd.ie/bitstream/handle/2262/90913/TCD-CS-2007-27.pdf},
  DATE = {2007-07},
  NOTE = {URI: http://hdl.handle.net/2262/90913},
  NUMBER = {TCD-CS-2007-27},
  TITLE = {Restoration of colour channel misalignments in colonoscopy videos},
  TYPE = {techreport},
}

@INPROCEEDINGS{Kelly2007,
  ABSTRACT = {The use of multiple audio streams from digital mixing consoles is presented for application to real-time enhancement of synchronised visual effects in live music performances. The audio streams are processed simultaneously and their temporal and spectral characteristics can be used to control the intensity, duration and colour of the lights. The efficiency of the approach is tested on rock and jazz pieces. The result of the analysis is illustrated by a visual OpenGL 3-D animation illustrating the synchronous audio-visual events occurring in the musical piece.},
  AUTHOR = {Dahyot, R. and Kelly, C. and Kearney, G.},
  LOCATION = {London, UK},
  URL = {https://www.aes.org/e-lib/browse.cfm?elib = 13947},
  BOOKTITLE = {31st International Conference Audio Engineering Society},
  DATE = {2007-06},
  EPRINT = {https://www.aes.org/e-lib/browse.cfm?elib = 13947},
  TITLE = {Visual enhancement using multiple audio streams in live music performance},
}

@INPROCEEDINGS{Dahyot_IWSM2006,
  AUTHOR = {Dahyot, R.},
  LOCATION = {Galway, Ireland},
  BOOKTITLE = {21st International Workshop on Statistical Modelling},
  DATE = {2006-07},
  PAGES = {127--130},
  TITLE = {Bayesian Inferences for Object Detection},
}

@ARTICLE{Kokaram2006,
  ABSTRACT = {This paper aims to identify the current trends in sports-based indexing and retrieval work. It discusses the essential building blocks for any semantic-level retrieval system and acts as a case study in content analysis system design. While one of the major benefits of digital media and digital television in particular has been to provide users with more choices and a more interactive viewing experience, the freedom to choose has in fact manifested as the freedom to choose from the options the broadcaster provides. It is only through the use of automated content-based analysis that sports viewers will be given a chance to manipulate content at a much deeper level than that intended by broadcasters, and hence put true meaning into interactivity},
  AUTHOR = {Kokaram, A. and Rea, N. and Dahyot, R. and Tekalp, M. and Bouthemy, P. and Gros, P. and Sezan, I.},
  URL = {http://www.tara.tcd.ie/bitstream/handle/2262/1998/01621448.pdf},
  DATE = {2006-03},
  DOI = {10.1109/MSP.2006.1621448},
  ISSN = {1053-5888},
  JOURNALTITLE = {IEEE Signal Processing Magazine},
  KEYWORDS = {content-based retrieval;indexing;sport;video retrieval;automated content-based analysis;content analysis system design;digital media;digital television;interactive viewing experience;retrieval work;semantic-level retrieval system;sports video;sports-based indexing;sports-related indexing;Broadcasting;Cameras;Educational institutions;Games;Image analysis;Indexing;Information retrieval;Multimedia communication;Packaging;Tagging},
  NOTE = {URI: http://hdl.handle.net/2262/1998},
  NUMBER = {2},
  PAGES = {47--58},
  TITLE = {Browsing sports video: trends in sports-related indexing and retrieval work},
  VOLUME = {23},
}

@INPROCEEDINGS{Rea2006,
  AUTHOR = {Rea, N. and Lambe, C. and Lacey, G. and Dahyot, R.},
  BOOKTITLE = {The 3rd European Conference on Visual Media Production (CVMP 2006) - Part of the 2nd Multimedia Conference 2006},
  DATE = {2006-11},
  DOI = {10.1049/cp:20061978},
  EPRINT = {https://ieeexplore.ieee.org/document/4156017/},
  ISSN = {0537-9989},
  PAGES = {106--114},
  TITLE = {Multimodal Periodicity Analysis for Illicit Content Detection in Videos},
}

@ARTICLE{Dahyot_MZ2006,
  ABSTRACT = {This article proposes a robust way to estimate the scale parameter of a generalised centered Gaussian mixture. The principle relies on the association of samples of this mixture to generate samples of a new variable that shows relevant distribution properties to estimate the unknown parameter. In fact, the distribution of this new variable shows a maximum that is linked to this scale parameter. Using nonparametric modelling of the distribution and the MeanShift procedure, the relevant peak is identified and an estimate is computed. The whole procedure is fully automatic and does not require any prior settings. It is applied to regression problems, and digital data processing.},
  AUTHOR = {Dahyot, R. and Wilson, S.},
  URL = {http://www.tara.tcd.ie/bitstream/handle/2262/8718/dahyot.pdf},
  DATE = {2006},
  JOURNALTITLE = {Advances in Methodology and Statistics (Metodološki zvezki)},
  NOTE = {Also at http://mrvar.fdv.uni-lj.si/pub/mz/mz3.1/dahyot.pdf},
  NUMBER = {1},
  PAGES = {21--37},
  TITLE = {Robust Scale Estimation for the generalized Gaussian Probability Density Function},
  VOLUME = {3},
}

@INPROCEEDINGS{Dahyot_IMVIP06,
  ABSTRACT = {In this article, we consider the robust estimation of a location parameter using Mestimators. We propose here to couple this estimation with the robust scale estimate proposed in [Dahyot and Wilson, 2006]. The resulting procedure is then completely unsupervised. It is applied to camera motion estimation and moving object detection in videos. Experimental results on different video materials show the adaptability and the accuracy of this new robust approach.},
  AUTHOR = {Dahyot, R.},
  LOCATION = {Dublin, Ireland},
  URL = {http://www.tara.tcd.ie/bitstream/handle/2262/2058/RzDimvip06.pdf},
  BOOKTITLE = {Irish Machine Vision and Image Processing conference (IMVIP 2006)},
  DATE = {2006},
  TITLE = {Unsupervised Camera Motion Estimation and Moving Object Detection in Videos},
}

@PROCEEDINGS{Cord2005,
  ABSTRACT = {Machine Learning (ML) techniques are used in situations where data is available in electronic format and ML algorithms can add value by analysing this data. This is the situation with the processing of multimedia content. The added value from ML can take a number of forms: by providing insight into the domain from which the data is drawn, by improving the performance of another process that is manipulating the data, by organising the data in some way or by helping to interpret multimedia content to make it more understandable. This potential for ML to add value in processing of multimedia content has made this one of the most popular application areas for ML research. Multimedia content has some characteristics that place specific demands on ML. The data is typically of very high dimension and dimension reduction is often required. The normal distinction between supervised and unsupervised techniques doesnt always apply; it is often the case that only some of the data is labeled or the user may assist in labeling the data during processing. Typically the ML process is preceded by a feature extraction stage and the success of the ML stage will often depend on the feature extraction. This workshop on Machine Learning Techniques for Processing Multimedia Content has been organized because of these special issues that arise with multimedia data. We have papers describing applications in image processing, video analysis and music classification. The research described in these papers has drawn on a wide range of ML techniques. It is hoped that this workshop will help identify important research directions for Machine Learning that will help in the processing of multimedia content.},
  EDITOR = {Cord, Matthieu and Cunningham, Padraig and Dahyot, Rozenn and Sziranyi, Tamas},
  LOCATION = {Bonn, Germany},
  URL = {http://www.tara.tcd.ie/bitstream/handle/2262/52985/Workshop2005.pdf},
  DATE = {2005-08},
  NOTE = {URI: http://hdl.handle.net/2262/52985},
  TITLE = {Proceedings of the Workshop on Machine Learning Techniques for Processing Multimedia Content},
}

@INPROCEEDINGS{ReaICIP05,
  ABSTRACT = {This paper investigates the semantic analysis of broadcast tennis footage. We consider the spatio-temporal behaviour of an object in the footage as being the embodiment of a semantic event. This object is tracked using a colour based particle filter. The video syntax and audio features are used to help delineate the temporal boundaries of these events. For broadcast tennis footage, the system firstly parses the video sequence based on the geometry of the content in view and classifies the clip as a particular view type. The temporal behaviour of the serving player is modelled using a HMM. As a result, each model is representative of a particular semantic episode. Events are then summarised using a number of synthesised keyframes.},
  AUTHOR = {Rea, N. and Dahyot, R. and Kokaram, A.},
  URL = {http://www.tara.tcd.ie/bitstream/handle/2262/19779/01530614.pdf},
  BOOKTITLE = {IEEE International Conference on Image Processing 2005},
  DATE = {2005-09},
  DOI = {10.1109/ICIP.2005.1530614},
  ISSN = {1522-4880},
  KEYWORDS = {image classification;image colour analysis;image representation;image sequences;particle filtering (numerical methods);video signal processing;broadcast tennis videos;particle filter;semantic analysis;spatio-temporal behaviour;video classification;video representation;video sequence;Content based retrieval;Educational institutions;Hidden Markov models;Histograms;Multimedia communication;Particle filters;Particle tracking;Streaming media;TV broadcasting;Videos},
  NOTE = {URI: http://hdl.handle.net/2262/19779},
  PAGES = {III-1204--7},
  TITLE = {Classification and representation of semantic content in broadcast tennis videos},
  VOLUME = {3},
}

@INPROCEEDINGS{KokaramCBMI05,
  ABSTRACT = {Content based analysis has traditionally been posed in the context of identifying some material in response to a user query. This paper illustrates that given a content based analysis process that can identify semantic events in a sequence, that sequence can then be changed in various ways. A Motion Keyframe is presented to re-express the viewing of a sequence. The notion of content analysis for control of other media processing engines is introduced. Tennis footage is used to illustrate the ideas since sports in general contains strong contextual information.},
  AUTHOR = {Kokaram, A. and Pitie, F. and Dahyot, R. and Rea, N. and Yeterian, S.},
  LOCATION = {Riga, Latvia},
  URL = {http://www.tara.tcd.ie/bitstream/handle/2262/24739/cbmi05.pdf},
  BOOKTITLE = {proceedings of the IEEE workshop on Content Based Multimedia Indexing (CBMI'05)},
  DATE = {2005-06},
  NOTE = {URI: http://hdl.handle.net/2262/24739},
  TITLE = {Content Controlled Image Representation for Sports Streaming},
}

@INPROCEEDINGS{10.1145/1101826.1101857,
  ABSTRACT = {Discontinuities in any information bearing signal serve to represent much of the vital or interesting content in that signal. A sharp loud noise in a movie could be a gun, or something breaking. In sports like tennis, cricket or snooker/pool it would indicate a point scoring event. In both cases the discontinuity is likely to be semantically relevant without further inference being necessary, once a particular domain is adopted. This paper discusses the importance of temporal motion discontinuities in inferring events in visual media. Two particular application domains are considered: content based audio/video synchronisation and event spotting in observational Psychology.},
  AUTHOR = {Denman, Hugh and Doyle, Erika and Kokaram, Anil and Lennon, Daire and Dahyot, Rozenn and Fuller, Ray},
  LOCATION = {Hilton, Singapore},
  PUBLISHER = {Association for Computing Machinery},
  URL = {https://mural.maynoothuniversity.ie/15289/1/RD_exploiting.pdf},
  BOOKTITLE = {Proceedings of the 7th ACM SIGMM International Workshop on Multimedia Information Retrieval},
  DATE = {2005},
  DOI = {10.1145/1101826.1101857},
  ISBN = {1595932445},
  KEYWORDS = {event spotting,video retrieval,motion tracking,information retrieval,bayesian inference},
  PAGES = {183--192},
  SERIES = {MIR 05},
  TITLE = {Exploiting Temporal Discontinuities for Event Detection and Manipulation in Video Streams},
}

@INPROCEEDINGS{PitieICCV2005,
  ABSTRACT = {This article proposes an original method to estimate a continuous transformation that maps a N-dimensional distribution to another. The method is iterative, non-linear, and is shown to converge. Only 1D marginal distributions are used in the estimation process, hence involving low computation costs. As an illustration this mapping is applied to colour transfer between two images of different contents. The paper also serves as a central focal point for collecting together the research activity in this area and relating it to the important problem of Automated Colour Grading.},
  AUTHOR = {Pitie, F. and Kokaram, A. C. and Dahyot, R.},
  URL = {http://www.tara.tcd.ie/bitstream/handle/2262/19800/01544887.pdf},
  BOOKTITLE = {Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1},
  DATE = {2005-10},
  DOI = {10.1109/ICCV.2005.166},
  ISSN = {1550-5499},
  KEYWORDS = {image colour analysis;probability;1D marginal distribution;automated color grading;color transfer;continuous transformation;probability density function;Color;Computational efficiency;Density functional theory;Distributed computing;Educational institutions;Image converters;Iterative methods;Rendering (computer graphics);Statistical distributions;Statistics},
  NOTE = {URI: http://hdl.handle.net/2262/19800 - Github: https://github.com/frcs/colour-transfer},
  PAGES = {1434--1439},
  TITLE = {N-dimensional probability density function transfer and its application to color transfer},
  VOLUME = {2},
}

@INPROCEEDINGS{PitieICIP05,
  ABSTRACT = {This paper presents a probabilistic framework for off-line multiple object tracking. At each timestep, a small set of deterministic candidates is generated which is guaranteed to contain the correct solution. Tracking an object within video then becomes possible using the Viterbi algorithm. In contrast with particle filter methods where candidates are numerous and random, the proposed algorithm involves a few candidates and results in a deterministic solution. Moreover, we consider here off-line applications where past and future information is exploited. This paper shows that, although basic and very simple, this candidate selection allows the solution of many tracking problems in different real-world applications and offers a good alternative to particle filter methods for off-line applications.},
  AUTHOR = {Pitie, F. and Berrani, S. A. and Kokaram, A. and Dahyot, R.},
  URL = {http://www.tara.tcd.ie/bitstream/handle/2262/19821/01530340.pdf},
  BOOKTITLE = {IEEE International Conference on Image Processing 2005},
  DATE = {2005-09},
  DOI = {10.1109/ICIP.2005.1530340},
  ISSN = {1522-4880},
  KEYWORDS = {maximum likelihood estimation;object detection;particle filtering (numerical methods);Viterbi algorithm;candidate selection;deterministic solution;off-line multiple object tracking;particle filter methods;probabilistic framework;Data mining;Feature extraction;Image sequences;Indexing;Information retrieval;Particle filters;Particle tracking;Performance analysis;Surveillance;Viterbi algorithm},
  NOTE = {URI: http://hdl.handle.net/2262/19821},
  PAGES = {III-109--12},
  TITLE = {Off-line multiple object tracking using candidate selection and the Viterbi algorithm},
  VOLUME = {3},
}

@ARTICLE{Dahyot_PAA,
  ABSTRACT = {In this paper, we introduce a Bayesian approach, inspired by probabilistic principal component analysis (PPCA) (Tipping and Bishop in J Royal Stat Soc Ser B 61(3):611--622, 1999), to detect objects in complex scenes using appearance-based models. The originality of the proposed framework is to explicitly take into account general forms of the underlying distributions, both for the in-eigenspace distribution and for the observation model. The approach combines linear data reduction techniques (to preserve computational efficiency), non-linear constraints on the in-eigenspace distribution (to model complex variabilities) and non-linear (robust) observation models (to cope with clutter, outliers and occlusions). The resulting statistical representation generalises most existing PCA-based models (Tipping and Bishop in J Royal Stat Soc Ser B 61(3):611--622, 1999; Black and Jepson in Int J Comput Vis 26(1):63--84, 1998; Moghaddam and Pentland in IEEE Trans Pattern Anal Machine Intell 19(7):696--710, 1997) and leads to the definition of a new family of non-linear probabilistic detectors. The performance of the approach is assessed using receiver operating characteristic (ROC) analysis on several representative databases, showing a major improvement in detection performances with respect to the standard methods that have been the references up to now.},
  AUTHOR = {Dahyot, Rozenn and Charbonnier, Pierre and Heitz, Fabrice},
  URL = {https://mural.maynoothuniversity.ie/15128/1/RD_a%20bayesian.pdf},
  DATE = {2004-12-01},
  DOI = {10.1007/s10044-004-0230-5},
  ISSN = {1433-755X},
  JOURNALTITLE = {Pattern Analysis and Applications},
  NUMBER = {3},
  PAGES = {317--332},
  TITLE = {A Bayesian approach to object detection using probabilistic appearance-based models},
  VOLUME = {7},
}

@INPROCEEDINGS{Pitiesmvp2004,
  ABSTRACT = {Temporal random variation of luminance in images can manifest in film and video due to a wide variety of sources. Typical in archived films, it also affects scenes recorded simultaneously with different cameras (e.g. for film special effect), and scenes affected by illumination problems. Many applications in Computer Vision and Image Processing that try to match images (e.g. for motion estimation, stereo vision, etc.) have to cope with this problem. The success of current techniques for dealing with this is limited by the non-linearity of severe distortion, the presence of motion and missing data (yielding outliers in the estimation process) and the lack of fast implementations in reconfigurable systems. This paper proposes a new process for stabilizing brightness fluctuations that improves the existing models. The article also introduces a new estimation method able to cope with outliers in the joint distribution of pairs images. The system implementation is based on the novel use of general purpose PC graphics hardware. The overall system presented here is able to deal with much more severe distortion than previously was the case, and in addition can operate at 7 fps on a 1.6GHz PC with broadcast standard definition images.},
  AUTHOR = {Pitie, François and Dahyot, Rozenn and Kelly, Francis and Kokaram, Anil},
  EDITOR = {Comaniciu, Dorin and Mester, Rudolf and Kanatani, Kenichi and Suter, David},
  LOCATION = {Berlin, Heidelberg},
  PUBLISHER = {Springer Berlin Heidelberg},
  URL = {https://link.springer.com/content/pdf/10.1007/978-3-540-30212-4_14.pdf},
  BOOKTITLE = {Statistical Methods in Video Processing},
  DATE = {2004},
  DOI = {10.1007/978-3-540-30212-4{\_}14},
  ISBN = {978-3-540-30212-4},
  PAGES = {153--164},
  TITLE = {A New Robust Technique for Stabilizing Brightness Fluctuations in Image Sequences},
}

@INPROCEEDINGS{Dahyot_IMVIP04,
  ABSTRACT = {The estimation of Global or Camera motion from image sequences is important both for video retrieval and compression (MPEG4). This is frequently performed using robust M-estimators with the widely used Iterative Reweighted Least Squares algorithm. This article presents an investigation of the use of an alternative robust estimation algorithm and illustrates its improved computationnal efficiency. The paper also introduces two new confidence measures which can be used to validate camera motion measurements in the context of information retrieval.},
  AUTHOR = {Dahyot, R. and Kokaram, A.},
  LOCATION = {Dublin, Ireland},
  URL = {https://mural.maynoothuniversity.ie/15315/1/RD_comparison.pdf},
  BOOKTITLE = {Irish Machine Vision and Image Processing conference (IMVIP 2004)},
  DATE = {2004-09},
  KEYWORDS = {Camera motion,M-estimators,Video analysis},
  PAGES = {224--231},
  TITLE = {Comparison of Two Algorithms for Robust M-estimation of Global Motion Parameters},
}

@INPROCEEDINGS{DahyotMMSP04,
  ABSTRACT = {This paper presents a robust method to estimate the unknown standard deviation of a centred normal distribution from a mixture density. This method is applied to different signal processing problems. The first one concerns silence segmentation from audio data. The second application deals with colour class parameter extraction. In this later case, the mean is also estimated from the observations.},
  AUTHOR = {Dahyot, R. and Rea, N. and Kokaram, A. and Kingsbury, N.},
  URL = {http://www.tara.tcd.ie/bitstream/handle/2262/19839/01436600.pdf},
  BOOKTITLE = {IEEE 6th Workshop on Multimedia Signal Processing, 2004.},
  DATE = {2004-09},
  DOI = {10.1109/MMSP.2004.1436600},
  KEYWORDS = {audio signal processing;multimedia communication;normal distribution;audio data segmentation;centred normal distribution;colour class parameter extraction;multimedia data analysis;signal processing;Data analysis;Distributed computing;Educational institutions;Gaussian distribution;Parameter estimation;Parameter extraction;Random variables;Robustness;Signal processing;Statistical distributions},
  NOTE = {URI: http://hdl.handle.net/2262/19839},
  PAGES = {482--485},
  TITLE = {Inlier modeling for multimedia data analysis},
}

@INPROCEEDINGS{Rea_ICASSP04,
  ABSTRACT = {In this paper, we investigate the retrieval of dynamic events that occur in broadcast sports footage. Dynamic events in sports are important in so far as they are related to the game semantics. Thus far, the temporal interleaving of camera views has been used to infer these types of events. We propose the use of the spatio-temporal behaviour of an object in the footage as an embodiment of a semantic event. This is accomplished by modeling the evolution of the position of the object with a hidden Markov model (HMM). Snooker is used as an example for the purpose of this research. The system firstly parses the video sequence based on the geometry of the content in the camera view and classifies the footage as a particular view type. Secondly, we consider the relative position of the white ball on the snooker table over the duration of a clip to embody semantic events. A colour based particle filter is employed to robustly track the snooker balls. The temporal behaviour of the white ball is modeled using a HMM where each model is representative of a particular semantic episode. Upon collision of the white ball with another coloured ball, a separate track is instantiated.},
  AUTHOR = {Rea, N. and Dahyot, R. and Kokaram, A.},
  URL = {http://www.tara.tcd.ie/bitstream/handle/2262/24562/01326621.pdf},
  BOOKTITLE = {2004 IEEE International Conference on Acoustics, Speech, and Signal Processing},
  DATE = {2004-05},
  DOI = {10.1109/ICASSP.2004.1326621},
  ISSN = {1520-6149},
  KEYWORDS = {feature extraction;hidden Markov models;image recognition;image retrieval;motion estimation;sport;video signal processing;broadcast sports footage;collision detection;colour based particle filter;dynamic events retrieval;feature extraction;game semantics;hidden Markov model;motion driven HMM;motion extraction;object position evolution modeling;semantic event recognition;snooker ball tracking;snooker table white ball position;sports high level structure modeling;view recognition;view type classification;Broadcasting;Cameras;Educational institutions;Games;Geometry;Hidden Markov models;Interleaved codes;Particle filters;Particle tracking;Video sequences},
  NOTE = {URI: http://hdl.handle.net/2262/24562},
  PAGES = {iii-621--4 vol.3},
  TITLE = {Modeling high level structure in sports with motion driven HMMs},
  VOLUME = {3},
}

@INPROCEEDINGS{PITIE_IMVIP04,
  AUTHOR = {Pitie, F. and Kokaram, A. and Dahyot, R.},
  LOCATION = {Dublin, Ireland},
  URL = {http://iprcs.org/pdf/IMVIP2004_Proceedings.pdf},
  BOOKTITLE = {Irish Machine Vision and Image Processing conference (IMVIP 2004)},
  DATE = {2004-09},
  PAGES = {158--165},
  TITLE = {Oriented Particle Spray: A New Probabilistic Contour Tracing with Directional Information},
}

@INPROCEEDINGS{ReaCIVR04,
  ABSTRACT = {In this paper we investigate the retrieval of semantic events that occur in broadcast sports footage. We do so by considering the spatio-temporal behaviour of an object in the footage as being the embodiment of a particular semantic event. Broadcast snooker footage is used as an example of the sports footage for the purpose of this research. The system parses the sports video using the geometry of the content in view and classifies the footage as a particular view type. A colour based particle filter is then employed to robustly track the snooker balls, in the appropriate view, to evoke the semantics of the event. Over the duration of a player shot, the position of the white ball on the snooker table is used to model the high level semantic structure occurring in the footage. Upon collision of the white ball with another coloured ball, a separate track is instantiated allowing for the detection of pots and fouls, providing additional clues to the event in progress.},
  AUTHOR = {Rea, N. and Dahyot, R. and Kokaram, A.},
  EDITOR = {Enser, Peter and Kompatsiaris, Yiannis and O'Connor, Noel E. and Smeaton, Alan F. and Smeulders, Arnold W. M.},
  LOCATION = {Berlin, Heidelberg},
  PUBLISHER = {Springer Berlin Heidelberg},
  URL = {https://link.springer.com/content/pdf/10.1007/978-3-540-27814-6_14.pdf},
  BOOKTITLE = {Image and Video Retrieval},
  DATE = {2004},
  DOI = {10.1007/978-3-540-27814-6{\_}14},
  ISBN = {978-3-540-27814-6},
  PAGES = {88--97},
  TITLE = {Semantic Event Detection in Sports Through Motion Understanding},
}

@BOOK{B-Dahyot03,
  AUTHOR = {Dahyot, Rozenn},
  LOCATION = {France},
  PUBLISHER = {Paris : Laboratoire Central des Ponts et Chaussées (LCPC) 2-7208-2028-1},
  URL = {https://roznn.github.io/PDF/mem_dahyot.pdf},
  DATE = {2003-09},
  NOTE = {(published in french)},
  SERIES = {Etudes et Recherches des Laboratoires des Ponts et Chaussées},
  TITLE = {Analyse d'images séquentielles de scènes routières par modèles d'apparence pour la gestion du réseau routier},
}

@ARTICLE{TS2003,
  ABSTRACT = {In this paper, methods are proposed to detect objects in complex scenes using statistical global appearance based models. In our approach, the standard eigenspace representation of a training image database and a priori non- Gaussian hypotheses are brought together in a Bayesian framework. This work unifies standard (appearancebased) detection methods already proposed in the literature and leads naturally to the definition of a new family of probabilistic detectors. It allows the use of more general a priori assumptions about the distribution on the eigenspace and its orthogonal. Experimental results are illustrated with ROC (Receiver Operating Characteristic) curves and show the major improvement of our Bayesian approach in comparison to the standard methods that have been the reference up to now [2, 14].},
  AUTHOR = {Dahyot, R. and Charbonnier, P. and Heitz, F.},
  URL = {http://documents.irevues.inist.fr/bitstream/handle/2042/2221/Charbonnier.pdf},
  DATE = {2003},
  JOURNALTITLE = {Traitement du Signal},
  NOTE = {HANDLE: http://hdl.handle.net/2042/2221},
  NUMBER = {2},
  PAGES = {101--117},
  TITLE = {Detection robuste par modele probabiliste d apparence : une approche bayesienne},
  VOLUME = {20},
}

@INPROCEEDINGS{DahyotICASSP03,
  ABSTRACT = {In recent years, there has been increasing work in the area of content retrieval for sports. The idea is generally to extract important events or create summaries to allow personalisation of the media stream. While previous work in sports analysis has employed either the audio or video stream to achieve some goal, there is little work that explores how much can be achieved by combining the two streams. This paper combines both audio and image features to identify the key episode in tennis broadcasts. The image feature is based on image moments and is able to capture the essence of scene geometry without recourse to 3D modelling. The audio feature uses PCA to identify the sound of the ball hitting the racket. The features are modelled as stochastic processes and the work combines the features using a likelihood approach. The results show that combining the features yields a much more robust system than using the features separately.},
  AUTHOR = {Dahyot, R. and Kokaram, A. and Rea, N. and Denman, H.},
  URL = {http://www.tara.tcd.ie/bitstream/handle/2262/81765/final_icassp03.pdf},
  BOOKTITLE = {Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03). 2003 IEEE International Conference on},
  DATE = {2003-04},
  DOI = {10.1109/ICASSP.2003.1199536},
  ISSN = {1520-6149},
  KEYWORDS = {audio coding;content-based retrieval;feature extraction;image retrieval;maximum likelihood estimation;principal component analysis;sport;stochastic processes;video coding;PCA;audio features;content retrieval;image features;image moments;joint audio visual retrieval;key episode identification;likelihood approach;scene geometry;sports;stochastic processes;tennis broadcasts;Broadcasting;Content based retrieval;Geometry;Layout;Multimedia communication;Principal component analysis;Robustness;Solid modeling;Stochastic processes;Streaming media},
  NOTE = {URI: http://hdl.handle.net/2262/81765},
  PAGES = {III-561--4 vol.3},
  TITLE = {Joint audio visual retrieval for tennis broadcasts},
  VOLUME = {3},
}

@INPROCEEDINGS{Kokaram_VCIP03,
  ABSTRACT = {Temporal and spatial random variation of luminance in images, or 'flicker' is a typical degradation observed in archived film and video. The underlying premise in typical flicker reduction algorithms is that each image must be corrected for a spatially varying gain and offset. These parameters are estimated in the stationary region of the image. Hence the performance of that algorithm depends crucially on the identification of stationary image regions. Position fluctuations are also a common artefact resulting in a random 'shake' of each film frame. For removing both, the key is to reject regions showing local motion or other outlier activity. Parameters are then estimated mostly on that part of the image undergoing the dominant motion. A new algorithm that simultaneously deals with global motion estimation and flicker is presented. The final process is based on a robust application of weighted least-squares, in which the weights also classify portions of the image as local or global. The paper presents results on severely degraded sequences showing evidence of both Flicker and random shake.},
  AUTHOR = {Kokaram, A. C. and Dahyot, R. and Pitie, F. and Denman, H.},
  URL = {https://roznn.github.io/PDF/vcip2003_kokaram_pitie.pdf},
  BOOKTITLE = {Proc.SPIE Visual Communications and Image Processing},
  DATE = {2003},
  DOI = {10.1117/12.476584},
  PAGES = {5022 - 5022 -- 12},
  TITLE = {Simultaneous Luminance and Position Stabilization for Film and Video},
  VOLUME = {5022},
}

@INPROCEEDINGS{Dahyot_VCIP03,
  ABSTRACT = {This paper considers the statistics of local appearance based measures that are suitable for the visual parsing of sport events. The moments of the colour information are computed, and the shape content in the frames is characterised by the moments of local shape measures. Their generation process is very low cost. The temporal evolution of the features then is modelled with a Hidden Markov Model. The HMM is used to generate higher level information by classifying the shots as close ups, court views, crowd shots and so on. The paper illustrates how those simple features, coupled with the HMM, can be used for parsing snooker and tennis footages.},
  AUTHOR = {Dahyot, Rozenn and Rea, Niall and Kokaram, Anil C.},
  URL = {http://www.tara.tcd.ie/bitstream/handle/2262/37046/Sport%20Video%20Shot.pdf},
  BOOKTITLE = {Proc. SPIE Visual Communications and Image Processing 2003},
  DATE = {2003},
  DOI = {10.1117/12.503127},
  PAGES = {5150 - 5150 -- 10},
  TITLE = {Sport video shot segmentation and classification},
  VOLUME = {5150},
}

@INPROCEEDINGS{PitieGRETSI2003,
  ABSTRACT = {La variation temporelle de la luminance dans les sequences d'images, ou effet de pompage, est une dégradation typique des archives videos et cinematographiques. Nous proposons ici un nouveau procede qui vise à supprimer ces perturbations visuellement désagréables. Plusieurs améliorations sont proposées à la fois sur le modèle de pompage, l'estimation des paramètres correspondants et sur la méthode de compensation des images. Les expériences menées sur des videos, dont l'une est particulièrement dégradée, permettent de montrer l'apport de notre système de restauration par rapport aux méthodes existantes.},
  AUTHOR = {Pitie, F. and Dahyot, R. and Kokaram, A.},
  LOCATION = {Paris, France},
  URL = {http://documents.irevues.inist.fr/bitstream/handle/2042/13630/A275.pdf},
  BOOKTITLE = {proceedings of GRETSI conference on signal and image processing},
  DATE = {2003-09},
  DOI = {2042/13630},
  TITLE = {Suppression du bruit de pompage dans les videos},
}

@INPROCEEDINGS{Dahyot_ISS02,
  AUTHOR = {Delacourt, P. and Kokaram, A. and Dahyot, R.},
  LOCATION = {Cork, Ireland},
  BOOKTITLE = {proceedings of Irish Signals and Systems Conference},
  DATE = {2002-06},
  TITLE = {Comparison of Global motion estimators},
}

@THESIS{Dahyot01,
  AUTHOR = {Dahyot, Rozenn},
  INSTITUTION = {University of Strasbourg I},
  LOCATION = {France},
  URL = {https://roznn.github.io/PDF/mem_dahyot.pdf},
  DATE = {2001-11},
  EPRINT = {http://theses.fr/2001STR13130},
  NOTE = {(published in French)},
  TITLE = {Analyse d'images séquentielles de scènes routières par modèles d'apparence pour la gestion du réseau routier (Appearance based road scene video analysis for the management of the road network)},
  TYPE = {phdthesis},
}

@INPROCEEDINGS{Dahyota_gretsi01_event,
  ABSTRACT = {La détection de changements dans les séquences d'images s'est principalement intéressée à la détection d'objets mobiles quand le système d'acquisition est statique, ou à la détection d'effets de production, comme les changements de plans. Lorsque la caméra est mobile, son mouvement est classiquement géré par compensation du mouvement dominant, ce qui met en oeuvre des techniques d'estimation de mouvement et/ou de segmentation. Dans cet article, nous proposons une nouvelle méthode de détection de changements statistiques capable de gérer des événements complexes tels que l'entrée ou la sortie d'objets, et le changement d'apparence d'objets quand la caméra est en mouvement. Les changements temporels sont extraits en analysant les distributions statistiques d'images successives. Si l'on considère des mesures appropriées, nous montrons comment extraire les statistiques des objets changeants en utilisant deux histogrammes d'images successives. Ces objets sont ensuite localisés par une technique de rétroprojection. La méthode est complètement non supervisée et ne nécessite ni estimation, ni compensation du mouvement. Elle est illustrée sur des images de scènes routières présentant de grands mouvements de caméra.},
  AUTHOR = {Dahyot, R. and Charbonnier, P. and Heitz, F.},
  LOCATION = {Toulouse, France},
  URL = {http://documents.irevues.inist.fr/bitstream/handle/2042/13333/PAPER188.pdf},
  BOOKTITLE = {proceedings of GRETSI conference on signal and image processing},
  DATE = {2001-09},
  DOI = {2042/13333},
  TITLE = {Détection d'événements dans les séquences d'images avec caméra en mouvement},
}

@INPROCEEDINGS{Dahyot_gretsi01_robust,
  ABSTRACT = {Les méthodes classiques de détection basées sur la représentation de l'apparence par espace propre sont sensibles à la présence d'erreurs grossières dans les observations, induites, par exemple, par des occultations. Récemment, l'utilisation de techniques issues des statistiques robustes, les M-estimateurs, ont permis de gérer la présence de ces données erronées dans le cadre de la reconnaissance d'objets. Nous proposons dans cet article d'étendre cette approche robuste pour définir deux nouveaux détecteurs, capables de localiser les occurrences dégradées ou occultées d'objets d'intérêt dans des scènes texturées.},
  AUTHOR = {Dahyot, R. and Charbonnier, P. and Heitz, F.},
  LOCATION = {Toulouse, France},
  URL = {http://documents.irevues.inist.fr/bitstream/handle/2042/13335/PAPER191.pdf},
  BOOKTITLE = {proceedings of GRETSI conference on signal and image processing},
  DATE = {2001-09},
  DOI = {2042/13335},
  TITLE = {Détection robuste d'objets : une approche par modele d'apparence},
}

@INPROCEEDINGS{Dahyot_icip01,
  AUTHOR = {Dahyot, R. and Charbonnier, P. and Heitz, F.},
  URL = {https://github.com/Roznn/Detection-of-Changing-Objects-in-Camera-in-Motion-Video/blob/master/paper/htm_icip2001.pdf},
  BOOKTITLE = {Proceedings 2001 International Conference on Image Processing},
  DATE = {2001},
  DOI = {10.1109/ICIP.2001.959126},
  KEYWORDS = {feature extraction;image sequences;statistical analysis;backprojection;camera motion;camera-in-motion video;change detection;entering objects;exiting objects;image features;image histograms;image sequences;moving objects;object appearance;road scenes;unsupervised statistical detection;Cameras;Event detection;Gunshot detection systems;Image analysis;Image segmentation;Image sequences;Layout;Motion estimation;Object detection;Production systems},
  NOTE = {Github: https://github.com/Roznn/Detection-of-Changing-Objects-in-Camera-in-Motion-Video},
  PAGES = {638--641},
  TITLE = {Unsupervised statistical detection of changing objects in camera-in-motion video},
  VOLUME = {1},
}

@INPROCEEDINGS{Dahyot_cvpr00,
  ABSTRACT = {In this paper a robust pattern recognition system, using an appearance-based representation of colour images is described. Standard appearance-based approaches are not robust to outliers, occlusions or segmentation errors. The approach proposed here relies on robust M-estimators, involving non-quadratic and possibly non-convex energy functions. To deal with the minimisation of non-convex functions in a deterministic framework, we introduce an estimation scheme relying on M-estimators used in continuation, from convex functions to hard redescending nonconvex estimators. At each step of the robust estimation scheme, the non-quadratic criterion is minimized using the half-quadratic theory. This leads to a weighted least squares algorithm, which is easy to implement. The proposed robust estimation scheme does not require any user interaction because all necessary parameters are previously estimated. The method is illustrated on a road sign recognition application. Experiments show significant improvements with respect to standard estimation schemes.},
  AUTHOR = {Dahyot, R. and Charbonnier, P. and Heitz, F.},
  URL = {https://roznn.github.io/PDF/htm_Cvpr00.pdf},
  BOOKTITLE = {Proceedings IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  DATE = {2000},
  DOI = {10.1109/CVPR.2000.855886},
  ISSN = {1063-6919},
  KEYWORDS = {estimation theory;image recognition;image representation;appearance-based representation;colour images;pattern recognition;robust estimation;visual recognition;weighted least squares;Databases;Electrical capacitance tomography;Equations;Image recognition;Image reconstruction;Image segmentation;Least squares methods;Parameter estimation;Pattern recognition;Robustness},
  PAGES = {685--690 vol.1},
  TITLE = {Robust visual recognition of colour images},
  VOLUME = {1},
}

@INPROCEEDINGS{Dahyot_cbmi99,
  AUTHOR = {Dahyot, R. and Charbonnier, P. and Heitz, F.},
  LOCATION = {Toulouse, France},
  URL = {https://roznn.github.io/PDF/htm_cbmi99.pdf},
  BOOKTITLE = {proceedings of European Workshop on Content-Based Multimedia Indexing (CBMI)},
  DATE = {1999-10},
  TITLE = {Non-Supervised Robust Visual Recognition of Colour Images using Half-Quadratic Theory},
}

@INPROCEEDINGS{Dahyot_gretsi99,
  ABSTRACT = {Cet article décrit un système robuste de reconnaissance d'objets à partir d'images en couleur. Les méthodes usuelles basées sur l'apparence sont sensibles aux données erronées occasionnées par des occlusions ou des erreurs de segmentation. L'approche proposée ici utilise les M-estimateurs mettant en oeuvre des fonctions d'énergies non-quadratiques voire non-convexes. Pour minimiser ces fonctions non-convexes, nous présentons un système d'estimation utilisant les M-estimateurs en continuation, d'une fonction convexe vers des estimateurs non-convexes. À chaque étape de cette chaîne robuste, un critère non-quadratique est minimisé grâce à la théorie semi-quadratique. Ceci conduit à un algorithme de moindres carrés pondérés facile à implémenter, peu coûteux et non supervisé (tous les paramètres étant estimés automatiquement). Cette méthode est illustrée ici dans un problème de reconnaissance de panneaux routiers.},
  AUTHOR = {Dahyot, R. and Charbonnier, P. and Heitz, F.},
  LOCATION = {Vannes, France},
  URL = {http://documents.irevues.inist.fr/bitstream/handle/2042/12964/ARTI1293.pdf},
  BOOKTITLE = {proceedings of GRETSI conference on signal and image processing},
  DATE = {1999-09},
  DOI = {2042/12964},
  PAGES = {295--298},
  TITLE = {Reconnaissance robuste non supervisée d'images en couleur utilisant la théorie semi-quadratique},
  VOLUME = {2},
}

