<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 7 Deviance | Lecturenotes: Generalized Linear Models</title>
  <meta name="description" content="Lecturenotes">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 7 Deviance | Lecturenotes: Generalized Linear Models" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Lecturenotes" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Deviance | Lecturenotes: Generalized Linear Models" />
  
  <meta name="twitter:description" content="Lecturenotes" />
  

<meta name="author" content="Rozenn Dahyot">


<meta name="date" content="2019-03-06">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="sec-AIC.html">
<link rel="next" href="explanatory-variables-in-glms.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Generalized Linear Models</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Forewords</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#sec:LR:reminder"><i class="fa fa-check"></i><b>2.1</b> Linear Regression - Reminder</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#sec:LR:new"><i class="fa fa-check"></i><b>2.2</b> Linear Regression - Reformulation</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#sec:intro:glm"><i class="fa fa-check"></i><b>2.3</b> Generalised Linear Models</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="distributions-for-response-variable-y.html"><a href="distributions-for-response-variable-y.html"><i class="fa fa-check"></i><b>3</b> Distributions for response variable <span class="math inline">\(y\)</span></a><ul>
<li class="chapter" data-level="3.1" data-path="distributions-for-response-variable-y.html"><a href="distributions-for-response-variable-y.html#sec:exponential:family"><i class="fa fa-check"></i><b>3.1</b> Exponential family of distributions</a></li>
<li class="chapter" data-level="3.2" data-path="distributions-for-response-variable-y.html"><a href="distributions-for-response-variable-y.html#some-members-of-the-exponential-family"><i class="fa fa-check"></i><b>3.2</b> Some members of the exponential family</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>4</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="4.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#formal-structure-for-the-class-of-generalized-linear-models"><i class="fa fa-check"></i><b>4.1</b> Formal structure for the class of generalized Linear Models</a></li>
<li class="chapter" data-level="4.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#statistical-analysis-with-gmls"><i class="fa fa-check"></i><b>4.2</b> Statistical analysis with GMLs</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="binomialpoisson.html"><a href="binomialpoisson.html"><i class="fa fa-check"></i><b>5</b> Binomial &amp; Poisson Distributions</a><ul>
<li class="chapter" data-level="5.1" data-path="binomialpoisson.html"><a href="binomialpoisson.html#sec:offset:exposure"><i class="fa fa-check"></i><b>5.1</b> Offset and Exposure</a></li>
<li class="chapter" data-level="5.2" data-path="binomialpoisson.html"><a href="binomialpoisson.html#sec:poisson:binomial:n:infty"><i class="fa fa-check"></i><b>5.2</b> Relation between Poisson and Binomial distributions when <span class="math inline">\(n\rightarrow +\infty\)</span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="sec-AIC.html"><a href="sec-AIC.html"><i class="fa fa-check"></i><b>6</b> Akaike Information Criterion</a><ul>
<li class="chapter" data-level="6.1" data-path="sec-AIC.html"><a href="sec-AIC.html#likelihood-and-log-likelihood"><i class="fa fa-check"></i><b>6.1</b> Likelihood and log-likelihood</a></li>
<li class="chapter" data-level="6.2" data-path="sec-AIC.html"><a href="sec-AIC.html#comparing-working-models-with-the-aic"><i class="fa fa-check"></i><b>6.2</b> Comparing working models with the AIC</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="sec-deviance.html"><a href="sec-deviance.html"><i class="fa fa-check"></i><b>7</b> Deviance</a><ul>
<li class="chapter" data-level="7.1" data-path="sec-deviance.html"><a href="sec-deviance.html#deviance"><i class="fa fa-check"></i><b>7.1</b> Deviance</a></li>
<li class="chapter" data-level="7.2" data-path="sec-deviance.html"><a href="sec-deviance.html#approximation-of-the-log-likelihood-function-near-its-maximum"><i class="fa fa-check"></i><b>7.2</b> Approximation of the log likelihood function near its maximum</a></li>
<li class="chapter" data-level="7.3" data-path="sec-deviance.html"><a href="sec-deviance.html#sampling-distribution-for-the-deviance"><i class="fa fa-check"></i><b>7.3</b> Sampling distribution for the deviance</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="explanatory-variables-in-glms.html"><a href="explanatory-variables-in-glms.html"><i class="fa fa-check"></i><b>8</b> Explanatory variables in GLMs</a><ul>
<li class="chapter" data-level="8.1" data-path="explanatory-variables-in-glms.html"><a href="explanatory-variables-in-glms.html#nature-of-variables"><i class="fa fa-check"></i><b>8.1</b> Nature of variables</a></li>
<li class="chapter" data-level="8.2" data-path="explanatory-variables-in-glms.html"><a href="explanatory-variables-in-glms.html#generalized-mixed-linear-models"><i class="fa fa-check"></i><b>8.2</b> Generalized Mixed Linear Models</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="sec-survival.html"><a href="sec-survival.html"><i class="fa fa-check"></i><b>9</b> Survival Analysis</a><ul>
<li class="chapter" data-level="9.1" data-path="sec-survival.html"><a href="sec-survival.html#distributions"><i class="fa fa-check"></i><b>9.1</b> Distributions</a></li>
<li class="chapter" data-level="9.2" data-path="sec-survival.html"><a href="sec-survival.html#survivor-and-hazard-functions"><i class="fa fa-check"></i><b>9.2</b> Survivor and hazard functions</a><ul>
<li class="chapter" data-level="9.2.1" data-path="sec-survival.html"><a href="sec-survival.html#survivor-and-hazard-functions-for-the-exponential-distribution"><i class="fa fa-check"></i><b>9.2.1</b> Survivor and hazard functions for the exponential distribution</a></li>
<li class="chapter" data-level="9.2.2" data-path="sec-survival.html"><a href="sec-survival.html#survivor-and-hazard-functions-for-the-weibull-distribution"><i class="fa fa-check"></i><b>9.2.2</b> Survivor and hazard functions for the Weibull distribution</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="sec-survival.html"><a href="sec-survival.html#link-function"><i class="fa fa-check"></i><b>9.3</b> Link function</a></li>
<li class="chapter" data-level="9.4" data-path="sec-survival.html"><a href="sec-survival.html#estimation-of-beta-with-the-likelihood"><i class="fa fa-check"></i><b>9.4</b> Estimation of <span class="math inline">\(\beta\)</span> with the Likelihood</a><ul>
<li class="chapter" data-level="9.4.1" data-path="sec-survival.html"><a href="sec-survival.html#with-uncensored-data"><i class="fa fa-check"></i><b>9.4.1</b> With uncensored data</a></li>
<li class="chapter" data-level="9.4.2" data-path="sec-survival.html"><a href="sec-survival.html#with-censored-data"><i class="fa fa-check"></i><b>9.4.2</b> With censored data</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="sec-survival.html"><a href="sec-survival.html#proportional-hazard-models"><i class="fa fa-check"></i><b>9.5</b> Proportional hazard models</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="multiple-regression.html"><a href="multiple-regression.html"><i class="fa fa-check"></i><b>10</b> Multiple Regression</a><ul>
<li class="chapter" data-level="10.1" data-path="multiple-regression.html"><a href="multiple-regression.html#reminder-least-squares-algorithm"><i class="fa fa-check"></i><b>10.1</b> Reminder: Least Squares algorithm</a></li>
<li class="chapter" data-level="10.2" data-path="multiple-regression.html"><a href="multiple-regression.html#covariance-of-hatbeta"><i class="fa fa-check"></i><b>10.2</b> Covariance of <span class="math inline">\(\hat{\beta}\)</span></a></li>
<li class="chapter" data-level="10.3" data-path="multiple-regression.html"><a href="multiple-regression.html#case-study-carbohydrate-diet"><i class="fa fa-check"></i><b>10.3</b> Case study : Carbohydrate Diet</a><ul>
<li class="chapter" data-level="10.3.1" data-path="multiple-regression.html"><a href="multiple-regression.html#fitting-with-lm-and-glm"><i class="fa fa-check"></i><b>10.3.1</b> Fitting with lm() and glm()</a></li>
<li class="chapter" data-level="10.3.2" data-path="multiple-regression.html"><a href="multiple-regression.html#using-math"><i class="fa fa-check"></i><b>10.3.2</b> Using Math</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="sec-multinomial.html"><a href="sec-multinomial.html"><i class="fa fa-check"></i><b>11</b> Multinomial Distribution</a><ul>
<li class="chapter" data-level="11.1" data-path="sec-multinomial.html"><a href="sec-multinomial.html#from-binomial-distribution-to-multinomial-distribution"><i class="fa fa-check"></i><b>11.1</b> From Binomial Distribution to Multinomial Distribution</a></li>
<li class="chapter" data-level="11.2" data-path="sec-multinomial.html"><a href="sec-multinomial.html#multinomial-distribution-poisson-random-variables"><i class="fa fa-check"></i><b>11.2</b> Multinomial Distribution &amp; Poisson random variables</a></li>
<li class="chapter" data-level="11.3" data-path="sec-multinomial.html"><a href="sec-multinomial.html#nominal-logistic-regression"><i class="fa fa-check"></i><b>11.3</b> Nominal logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://www.scss.tcd.ie/Rozenn.Dahyot/" target="blank">Rozenn Dahyot</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lecturenotes: Generalized Linear Models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="sec:deviance" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Deviance</h1>
<div id="deviance" class="section level2">
<h2><span class="header-section-number">7.1</span> Deviance</h2>
<p>The deviance is a log-likelihood ratio statistics that compares the saturated model with the proposed GLM model. We have already introduced the following terms:</p>
<ul>
<li><p>the maximum likelihood estimates for the saturated model are computed by: <span class="math display">\[(\hat{\theta}_1,\cdots,\hat{\theta}_{N})=\arg\max \left \lbrace  \log \mathcal{L}(\theta_1,\cdots,\theta_{N})  \right \rbrace\]</span> and the value <span class="math inline">\(\log \mathcal{L}(\hat{\theta}_1,\cdots,\hat{\theta}_{N})\)</span> is therefore the maximum value of the log likelihood function of the saturated model.</p></li>
<li><p>When considering a generalised linear model, a link function <span class="math inline">\(g\)</span> is used to constraint the parameters such that <span class="math inline">\(\theta_i\propto g^{-1}(x_i^{T}\beta),\ \forall i=1,\cdots,N\)</span>. In this case the likelihood is written <span class="math inline">\(\mathcal{L}(\beta)\)</span> and the log likelihood is <span class="math inline">\(\log \mathcal{L}(\beta)\)</span>. The parameter <span class="math inline">\(\beta\)</span> has often a lower dimension than <span class="math inline">\(\theta s\)</span> (i.e. <span class="math inline">\(\dim(\beta)\leq N\)</span>) and the maximum likelihood estimate is computed such that: <span class="math display">\[\hat{\beta}=\arg\max\  \left\lbrace \log \mathcal{L}(\beta) \right\rbrace\]</span> The maximum log likelihood value for the GLM model is then <span class="math inline">\(\log \mathcal{L}(\hat{\beta})\)</span>.</p></li>
</ul>

<div class="definition">
<p><span id="def:Deviance" class="definition"><strong>Definition 7.1  </strong></span><strong>(Deviance)</strong> The deviance, also called the log-likelihood (ratio) statistic, is defined by: <span class="math display">\[D=2\ \left\lbrace\log \mathcal{L}(\hat{\theta}_1,\cdots,\hat{\theta}_{N}) -\log \mathcal{L}(\hat{\beta})) \right \rbrace= 2 \  \log\left(  \frac{ \mathcal{L}(\hat{\theta}_1,\cdots,\hat{\theta}_{N})}{\mathcal{L}(\hat{\beta})} \right)\]</span> where</p>
<ul>
<li><p><span class="math inline">\(\log \mathcal{L}(\hat{\theta}_1,\cdots,\hat{\theta}_{N})\)</span> is the maximum value of the log-likelihood function for the saturated model, and</p></li>
<li><p><span class="math inline">\(\log \mathcal{L}(\hat{\beta})\)</span> is the value of the log-likelihood function when fitting the model <span class="math inline">\(g(\mathbb{E}[y])=\mathbf{x}^T\pmb{\hat{\beta}}\)</span>.</p>
</div>
</li>
</ul>
<p>The deviance is given as an output in R when fitting GLMs.</p>
</div>
<div id="approximation-of-the-log-likelihood-function-near-its-maximum" class="section level2">
<h2><span class="header-section-number">7.2</span> Approximation of the log likelihood function near its maximum</h2>
<p>Using Taylor expansion, the log likelihood can be approximated near the maximum likelihood estimate</p>
<ul>
<li><p>for the saturated model with notation <span class="math inline">\((\theta_1,\cdots,\theta_N)^T=\theta\)</span>, when <span class="math inline">\(\theta\)</span> is close to <span class="math inline">\(\hat{\theta}\)</span>: <span class="math display">\[\log \mathcal{L}(\theta)\simeq\log \mathcal{L}(\hat{\theta})+ (\theta-\hat{\theta})^T \  \nabla_{\hat{\theta}} +\frac{1}{2}  (\theta-\hat{\theta})^T \mathrm{H}_{\hat{\theta}} \ (\theta-\hat{\theta})\]</span> with <span class="math inline">\(\nabla_{\hat{\theta}}\)</span> the gradient of the log likelihood function at <span class="math inline">\(\hat{\theta}\)</span> and <span class="math inline">\(\mathrm{H}_{\hat{\theta}}\)</span> the hessian matrix of the log likelihood function at <span class="math inline">\(\hat{\theta}\)</span> for the saturated model.</p></li>
<li><p>similarly for the GLM model, when <span class="math inline">\(\beta\)</span> is close to <span class="math inline">\(\hat{\beta}\)</span>: <span class="math display">\[\log \mathcal{L}(\beta)\simeq\log \mathcal{L}(\hat{\beta})+ (\beta-\hat{\beta})^T \  \nabla_{\hat{\beta}} + \frac{1}{2}  (\beta-\hat{\beta})^T \mathrm{H}_{\hat{\beta}} \ (\beta-\hat{\beta})\]</span></p></li>
</ul>
<p>In both case, the gradients <span class="math inline">\(\nabla_{\hat{\beta}}\)</span> and <span class="math inline">\(\nabla_{\hat{\theta}}\)</span> are zero-vectors (since <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{\theta}\)</span> are maxima of the log likelihoods !). So the deviance can be approximated by: <span class="math display">\[\begin{array}{ll}
D&amp;\simeq 2\ \left\lbrace\log \mathcal{L}(\hat{\theta}) -\log \mathcal{L}(\hat{\beta})) \right \rbrace\\
&amp;\\
&amp;\simeq 2\ \left\lbrace \log \mathcal{L}(\theta)  - \frac{1}{2}  (\theta-\hat{\theta})^T \mathrm{H}_{\hat{\theta}} \ (\theta-\hat{\theta})   - \log \mathcal{L}(\beta)  +\frac{1}{2}  (\beta-\hat{\beta})^T \mathrm{H}_{\hat{\beta}} \ (\beta-\hat{\beta}) \right\rbrace\\
&amp;\\
&amp;\simeq \underbrace{2\ \log\left ( \frac{\mathcal{L}(\theta)}{\mathcal{L}(\beta)}  \right)}_{\nu}  -(\theta-\hat{\theta})^T \mathrm{H}_{\hat{\theta}} \ (\theta-\hat{\theta}) +  (\beta-\hat{\beta})^T \mathrm{H}_{\hat{\beta}} \ (\beta-\hat{\beta})\\
\end{array}\]</span> The term <span class="math inline">\(\nu\)</span> is positive and it will be near zero if the GLM model fits the data almost as well the saturated model does. Note that Hessian matrices computed at the maxima, <span class="math inline">\(\mathrm{H}_{\hat{\beta}}\)</span> and <span class="math inline">\(\mathrm{H}_{\hat{\theta}}\)</span>, are negative.</p>
</div>
<div id="sampling-distribution-for-the-deviance" class="section level2">
<h2><span class="header-section-number">7.3</span> Sampling distribution for the deviance</h2>
<p>The likelihood for <span class="math inline">\(\beta\)</span> can be approximated by a Normal distribution near the estimate <span class="math inline">\(\hat{\beta}\)</span> such that <span class="math inline">\(\mathcal{L}(\beta)\propto p_{\beta}(\beta)\)</span> with: <span class="math display">\[\begin{array}{c}
p_{\beta}(\beta) =\frac{1}{\sqrt{2\pi |\Sigma|} } \exp\left\lbrack -\frac{1}{2} (\beta-\hat{\beta})^T \Sigma^{-1} (\beta-\hat{\beta})\right\rbrack\\
\text{or}\\
\log p_{\beta}(\beta) = -\log\left(  \sqrt{2\pi |\Sigma|}\right) -\frac{1}{2} (\beta-\hat{\beta})^T \Sigma^{-1} (\beta-\hat{\beta}) 
\end{array}\]</span> Comparing with <span class="math display">\[\log \mathcal{L}(\beta)\simeq\log \mathcal{L}(\hat{\beta})+ \frac{1}{2}  (\beta-\hat{\beta})^T \mathrm{H}_{\hat{\beta}} \ (\beta-\hat{\beta})\]</span> we can identify the covariance matrix with the Hessian matrix of the log likelihood computed at <span class="math inline">\(\hat{\beta}\)</span>: <span class="math display">\[\Sigma^{-1}=-\mathrm{H}_{\hat{\beta}}\]</span> Remember that the covariance matrix is a real positive definite symmetric matrix that can be rewritten by eigen decomposition as: <span class="math display">\[\Sigma=\mathrm{U}^T{\Lambda}\mathrm{U}\]</span> with the orthogonal matrix <span class="math inline">\(\mathrm{U}\)</span> and the diagonal matrix <span class="math inline">\(\Lambda=\Upsilon^2\)</span> collecting the positive eigenvalues of <span class="math inline">\(\Sigma\)</span>. In this case, we have <span class="math display">\[\begin{array}{ll}
(\theta-\hat{\theta})^T \Sigma^{-1} (\theta-\hat{\theta}) &amp;= (\mathrm{U}\ (\theta-\hat{\theta}))^T \Lambda^{-1} (\mathrm{U}\ (\theta-\hat{\theta}))\\
&amp; = (\Upsilon^{-1}\ \mathrm{U}\ (\theta-\hat{\theta}))^T \ \mathrm{I} \ (\Upsilon^{-1}\ \mathrm{U}\ (\theta-\hat{\theta}))  \\
\end{array}\]</span> with <span class="math inline">\(\mathrm{I}\)</span> is the identity matrix. So by changing variable <span class="math inline">\(\vec{z}=(\Upsilon^{-1}\ \mathrm{U}\ (\theta-\hat{\theta}))\)</span>, we have <span class="math inline">\(\vec{z}\sim\mathcal{N}(0,\mathrm{I})\)</span>.</p>

<div class="definition">
<span id="def:chisquare" class="definition"><strong>Definition 7.2  </strong></span><strong>(Central <span class="math inline">\(\chi^2\)</span> distribution)</strong> The central <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(k\)</span> degree of freedom is defined as the distribution of the sum of squares of <span class="math inline">\(k\)</span> independent random variables <span class="math inline">\(z_1\)</span>, ..., <span class="math inline">\(z_k\)</span> such that <span class="math inline">\(z_i\sim \mathcal{N}(0,1), \forall i=1,\cdots,k\)</span>: <span class="math display">\[x=\sum_{i=1}^k z_i^2     \quad \text{then }  x\sim \chi^2(k)\]</span> with the distribution <span class="math inline">\(\chi^2(k)\)</span> defined as: <span class="math display">\[p_{x}(x) = 
\left\lbrace\begin{array}{l}
\frac{1}{2^{k/2} \Gamma(k/2)} x^{k/2-1} \exp\left(\frac{-x}{2}\right)  \quad \text{when }  x\geq 0\\
0 \quad \text{otherwise}\\
\end{array}\right.\]</span> with the function <span class="math inline">\(\Gamma\)</span> defined as <span class="math display">\[\Gamma(k)=\int_{0}^{+\infty} t^{k-1} \ \exp(-t) \ dt\]</span> <span class="math inline">\(\Gamma(1/2)=\sqrt{\pi}\)</span>, <span class="math inline">\(\Gamma(1)=1\)</span> and <span class="math inline">\(\Gamma(y+1)=y\ \Gamma(y) \ \forall y\)</span>. The expectation of <span class="math inline">\(x\)</span> is <span class="math inline">\(\mathbb{E}[x]=k\)</span>.
</div>

<p>For the deviance,</p>
<ul>
<li><p>the term <span class="math inline">\(- (\theta-\hat{\theta})^T \mathrm{H}_{\hat{\theta}} \ (\theta-\hat{\theta})\)</span> is a weighted sum of squares of <span class="math inline">\(N\)</span> variables with distribution <span class="math inline">\(\mathcal{N}(0,1)\)</span>, then this term follows a <span class="math inline">\(\chi^2(\dim(\theta))\)</span> distribution with <span class="math inline">\(\dim(\theta)=N\)</span> for the saturated model</p></li>
<li><p>similarly, the term <span class="math inline">\(- (\beta-\hat{\beta})^T \mathrm{H}_{\hat{\beta}} \ (\beta-\hat{\beta})\)</span> will follow a <span class="math inline">\(\chi^2(\dim(\beta))\)</span> distribution with <span class="math inline">\(\dim(\beta)=p\)</span>.</p></li>
</ul>

<div class="definition">
<span id="def:samplingdist" class="definition"><strong>Definition 7.3  </strong></span><strong>(Sampling distribution of the deviance)</strong> The sampling distribution of the deviance is approximated by a <span class="math inline">\(\chi^2\)</span> distribution of degree <span class="math inline">\(\dim(\theta)-\dim(\beta)\)</span> and non-centrality parameter <span class="math inline">\(\nu\)</span>: <span class="math display">\[D\sim \chi^2(\dim(\theta)-\dim(\beta),\nu)\]</span> with <span class="math inline">\(\dim(\theta)=N\)</span> the nb of parameters in the saturated model, <span class="math inline">\(\dim(\beta)=p\)</span> is the number of parameters in the GLM model of interest, and <span class="math inline">\(\nu\)</span> is a positive constant near 0 when the model of interest fits almost as well as the saturated model: <span class="math display">\[D\sim \chi^2(N-p,0)\]</span>
</div>

<!-- 
**Example.**

Once you have fitted a model (e.g. for $N=8$ response variables and with
$p=2$ parameters $(\beta_0,\beta_1)$ in the Beetles case), read in the
statistics tables to find $D_{95}$ (cf. fig.
[1.1](#fig:chi2:deviance){reference-type="ref"
reference="fig:chi2:deviance"}) and compare the deviance computed with
your model with $D_{95}$:

-   if $D<D_{95}$ accept the model,

-   if $D>D_{95}$ reject the model,

-   you may decide to look for a better model also when $D$ is close to
    $D_{95}$.

![$\chi^2(6)$ distribution. Integration of $\chi^2(6)$ between 0 and
$D_{95}$ is equal to 0.95 (the value of $D.95\simeq  12.59159$ and this
can be found in the statistical tables or with R). In other words, the
interval $[0;D_{95}]$ is the 95% confidence interval.
[]{label="fig:chi2:deviance"}](images/chi2d6){#fig:chi2:deviance
width=".5\linewidth"}

### Exercises {#exercises .unnumbered}

1.  Find the deviance for the following distributions used with their
    canonical link functions:

    1.  Binomial

    2.  Poisson

    3.  Normal

2.  Assuming that $z_1\sim \mathcal{N}(0,1)$, show that $x=z_1^2$ has a
    $\chi^2(1)$ distribution.

3.  Assuming that $z_1\sim \mathcal{N}(0,1)$ and
    $z_2\sim \mathcal{N}(0,1)$ such that $z_1$ and $z_2$ are
    independent, show that $x=z_1^2+z_2^2$ has a $\chi^2(2)$
    distribution.

4.  Given $x\sim \chi^2(k)$, show that $\mathbb{E}[x]=k$.

### Solutions {#solutions .unnumbered}

1.  Binomial: log-likelihood of saturated model:
    $$\log \mathcal{L}(\theta_1,\cdots,\theta_N)=\sum_{i=1}^{N} y_i \ \log\left( \frac{\theta_i}{1-\theta_i}\right)+n_i\ \log (1-\theta_i) + \log \left( \frac{n_i!}{(n_i-y_i)! y_i!} \right)$$
    At $\hat{\theta}_{i}=\frac{y_i}{n_i}$ (MLE):
    $$\log \mathcal{L}(\hat{\theta}_1,\cdots,\hat{\theta}_N)=\sum_{i=1}^{N} y_i \ \log\left( \frac{y_i}{n_i-y_i}\right)+n_i\ \log \left( \frac{n_i-y_i}{n_i} \right) + \log \left( \frac{n_i!}{(n_i-y_i)! y_i!} \right) 
    \label{eq:bin:dev:a}$$ For the fitted model, the link function $g$
    is the logit function
    $$\log \mathcal{L}(\beta)=\sum_{i=1}^{N} y_i \ \log\left( \frac{\theta_i}{1-\theta_i}\right)+n_i\ \log (1-\theta_i) + \log \left( \frac{n_i!}{(n_i-y_i)! y_i!} \right)  \quad \text{with} \ g(\theta_i)=x_i^T\beta$$
    At MLE $\hat{\beta}$, the fitted values by this model are noted
    $\hat{y}_i=\mathbb{E}[y_i]=n_i \ \hat{\theta}_i^{\beta} =n_i \ g^{-1}(x_i^T\hat{\beta})$
    or $\hat{\theta}^{\beta}_i=\frac{\hat{y}_{i}}{n_i}$.
    $$\log \mathcal{L}(\beta)=\sum_{i=1}^{N} y_i \ \log\left( \frac{\hat{y}_i}{n_i-\hat{y}_i}\right)+n_i\ \log \left( \frac{n_i-\hat{y}_i}{n_i} \right) + \log \left( \frac{n_i!}{(n_i-y_i)! y_i!} \right) 
    \label{eq:bin:dev:b}$$ The deviance is computed using the difference
    between equations
    [\[eq:bin:dev:a\]](#eq:bin:dev:a){reference-type="ref"
    reference="eq:bin:dev:a"} and
    [\[eq:bin:dev:b\]](#eq:bin:dev:b){reference-type="ref"
    reference="eq:bin:dev:b"}:
    
    
    -->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec-AIC.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="explanatory-variables-in-glms.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["GLM2.pdf", "GLM2.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
